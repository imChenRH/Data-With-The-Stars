# 数据预处理模块
## MCM 2026 Problem C: Data With The Stars

> **设计原则**：本模块提供完整的数据预处理方案，包含可执行Python代码，配合详细注释和高质量可视化，确保数据处理过程透明可复现。
>
> **核心理念**：本题是典型的**逆向推断问题（Inverse Problem）**——我们拥有系统的输入的一半（评委评分 J）和系统的最终输出（淘汰结果 E），却缺失了最关键的中间变量——粉丝投票 F。数据预处理的目标是为后续的约束优化/贝叶斯MCMC反演提供干净、结构化的数据输入。

---

## 一、数据集信息总览

### 1.1 数据集基本信息

| 属性 | 描述 |
|------|------|
| **数据文件** | `2026_MCM_Problem_C_Data.csv` |
| **数据维度** | 421行 × 53列 |
| **数据类型** | 结构化面板数据（Panel Data） |
| **时间跨度** | Season 1-34（约17年，2005-2022） |
| **数据格式** | CSV（UTF-8编码） |
| **数据性质** | 每行=一位选手在一个赛季的完整比赛记录 |

### 1.2 字段详细说明

#### 1.2.1 名人基本信息字段（9列）

| 字段名 | 数据类型 | 缺失值 | 说明 | 建模用途 |
|--------|----------|--------|------|----------|
| `celebrity_name` | String | 0 | 名人姓名 | 唯一标识符 |
| `ballroom_partner` | String | 0 | 专业舞伴姓名 | **问题三核心变量**：舞伴影响分析（如Derek Hough自带流量） |
| `celebrity_industry` | String | 0 | 名人所属行业（26类） | **问题三**：行业影响分析（运动员/演员/歌手等） |
| `celebrity_homestate` | String | 56 | 家乡州（美国） | 辅助变量：地域粉丝基础 |
| `celebrity_homecountry/region` | String | 0 | 家乡国家/地区 | 辅助变量：国际选手标记 |
| `celebrity_age_during_season` | Int64 | 0 | 参赛时年龄 | **问题三**：年龄影响分析（粉丝偏好） |
| `season` | Int64 | 0 | 参赛季数（1-34） | **关键分组变量**：规则切换依据 |
| `results` | String | 0 | 比赛结果（18种） | 淘汰周次提取（核心约束条件） |
| `placement` | Int64 | 0 | 最终排名（1=冠军） | **因变量**：建模目标 |

#### 1.2.2 评委评分字段（44列）

| 字段模式 | 数据类型 | 说明 | 特殊值处理 |
|----------|----------|------|------------|
| `weekX_judge1_score` | Float64 | 第X周评委1评分（1-10分） | 0=被淘汰后/未参赛 |
| `weekX_judge2_score` | Float64 | 第X周评委2评分 | 0=被淘汰后/未参赛 |
| `weekX_judge3_score` | Float64 | 第X周评委3评分 | 0=被淘汰后/未参赛 |
| `weekX_judge4_score` | Float64 | 第X周评委4评分（部分季） | NaN=该季无第4评委 |

**评委人数变化规律**：
```
Season 1-18:  3位评委（满分30分）
Season 19-20: 4位评委（满分40分）
Season 21-22: 3位评委（满分30分）
Season 23-24: 4位评委（满分40分）
Season 25-29: 3位评委（满分30分）
Season 30-31: 4位评委（满分40分）
Season 32-34: 3位评委（满分30分）
```

### 1.3 数据特殊值解读（⚠️建模雷区）

| 特殊值 | 含义 | 处理策略 | 影响建模 |
|--------|------|----------|----------|
| `0` | 选手**已被淘汰**或该周未参赛 | 标记后**排除**，分母只计算当周仍在比赛的选手 | 问题一：0分选手不参与该周投票逆推 |
| `NaN` | 该季无第4位评委 | 动态识别评委数量 | 归一化时需适配满分 |
| 小数评分（如8.5） | 某些季节允许半分 | 保留原值 | 不能假设分数必须是整数 |
| 复杂小数（如9.6666） | 多轮舞蹈平均分或Bonus直接加入 | 保留原值，直接信赖给定数据 | 不需额外处理Bonus逻辑 |

---

## 二、⚠️ 投票规则变迁时间线（核心约束条件）

> **重要提醒**：这是建模最容易出错的地方！代码必须能够根据 `season` 字段**自动切换计算逻辑**。

### 2.1 三阶段规则详解

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                         DWTS 投票规则变迁时间线                                   │
├──────────────┬──────────────────────────────────────────────────────────────────┤
│  阶段一      │  排名制 (Rank System) — Season 1-2                               │
│  (S1-S2)     │  ─────────────────────────────────────────────────────────────  │
│              │  公式: TotalRank = Rank(Judge) + Rank(FanVotes)                  │
│              │  淘汰: TotalRank **最大者**被淘汰（排名数字越小越好）               │
│              │  示例: 若4人中某人评委排第4+粉丝排第3=7，则若最大则被淘汰           │
│              │  特点: 分数差距被**压缩**，只保留序数信息（低通滤波器）             │
├──────────────┼──────────────────────────────────────────────────────────────────┤
│  阶段二      │  百分比制 (Percentage System) — Season 3-27                      │
│  (S3-S27)    │  ─────────────────────────────────────────────────────────────  │
│              │  公式: TotalScore = 50% × Pct(Judge) + 50% × Pct(FanVotes)       │
│              │  其中: Pct(Judge) = 本人评委分 / 当周所有选手评委分之和            │
│              │  淘汰: TotalScore **最小者**被淘汰                                │
│              │  特点: 分数差距被**保留**（线性放大器），粉丝优势可覆盖评委劣势     │
│              │  案例: Bobby Bones（S27冠军）技术分垫底但粉丝碾压，争议极大        │
├──────────────┼──────────────────────────────────────────────────────────────────┤
│  阶段三      │  混合制 + 评委拯救 (Rank + Judges' Save) — Season 28-34          │
│  (S28-S34)   │  ─────────────────────────────────────────────────────────────  │
│              │  计算: 恢复**排名制**计算综合排名                                  │
│              │  关键差异⚠️: 综合排名倒数两名(Bottom Two)进入"生死PK"              │
│              │            最终淘汰谁由**评委现场投票**决定，不再由排名决定！       │
│              │  建模影响: 若某人综合排名倒数第二但未被淘汰，是合理的（评委救了他） │
│              │           被淘汰者必须是"倒数两名之一"                            │
└──────────────┴──────────────────────────────────────────────────────────────────┘
```

### 2.2 问题一建模约束对照

| 阶段 | 季节 | 逆向推断约束条件 | Python伪代码 |
|------|------|------------------|--------------|
| **排名制** | S1-2, S28-34 | 被淘汰者 `Rank(J) + Rank(F)` 必须**最大** | `all(rank_sum[elim] >= rank_sum[j] for j != elim)` |
| **百分比制** | S3-27 | 被淘汰者 `0.5*Pct(J) + 0.5*F` 必须**最小** | `all(total[elim] <= total[j] for j != elim)` |
| **评委拯救** | S28-34 | 被淘汰者必须是倒数两名之一，且被评委**选择淘汰** | 需考虑Bottom Two逻辑 |

### 2.3 平局处理假设

题目未说明平局规则，建模时需做以下**合理假设**（论文需论证）：
- **排名制平局**：若综合排名相同，粉丝投票少的被淘汰
- **百分比制平局**：若总分相同，评委分低的被淘汰
- **建议**：在代码中添加微小扰动（如 `- 0.001`）避免严格平局

---

## 三、预处理必要性判断

### 3.1 预处理判断结论

**结论：需要进行预处理**

### 3.2 判断依据（⚠️数据清洗雷区详解）

| 问题类型 | 具体表现 | 影响程度 | 处理优先级 | 雷区说明 |
|----------|----------|----------|------------|----------|
| **0分标记问题** | 被淘汰者后续周评分为0，非真实缺失 | ⭐⭐⭐⭐⭐ | 最高 | 计算Pct(J)时分母只能包含**当周仍在比赛**的人（分数>0） |
| **评委数量变化** | 部分季节有4位评委（满分40分） | ⭐⭐⭐⭐⭐ | 最高 | 不能直接用总分比较，必须**归一化**为占满分百分比 |
| **规则切换逻辑** | 三个阶段规则完全不同 | ⭐⭐⭐⭐⭐ | 最高 | 代码必须根据`season`自动切换计算逻辑 |
| **多人淘汰/无人淘汰** | 部分周淘汰0人或2+人 | ⭐⭐⭐⭐ | 高 | 约束方程数量不固定，需动态处理 |
| **N/A值处理** | 代表缺席评委或休赛周 | ⭐⭐⭐ | 中 | 读取数据时必须剔除 |
| **文本字段编码** | `results`需解析为淘汰周次 | ⭐⭐⭐⭐ | 高 | 提取"Eliminated Week X"中的数字 |
| **缺失值处理** | `homestate`有56个缺失（非美国选手） | ⭐⭐ | 低 | 非核心变量，可填充"International" |
| **类别编码** | `industry`存在大小写不一致 | ⭐⭐ | 低 | 统一转换为Title Case |

### 3.3 各问题数据需求对照

| 问题 | 核心数据需求 | 预处理要点 | 输出数据结构 |
|------|--------------|------------|--------------|
| **问题一** | 每周每人评委占比、淘汰结果 | 0分标记→有效周识别→评委分归一化→狄利克雷先验准备 | `week_data[season][week] = {contestants, J_pct, elim_idx}` |
| **问题二** | 跨季节对比数据（反事实推演） | 统一评分量纲，准备两种规则的模拟器输入 | 同一数据在两种规则下的模拟结果 |
| **问题三** | 选手特征与表现关联 | 特征编码+舞伴ID化+SHAP可解释性输入 | `X_features`, `y_fan_votes`, `y_judge_scores` |
| **问题四** | 全量历史数据 | 综合清洗后的完整数据集 | `df_clean`, `df_weekly_long` |

---

## 三、完整预处理代码

### 3.1 代码结构说明

```
代码结构：
├── 第1部分：环境配置与数据加载
├── 第2部分：数据探索性分析（EDA）可视化
├── 第3部分：数据清洗
│   ├── 3.1 0分标记处理
│   ├── 3.2 评委数量动态识别
│   ├── 3.3 淘汰周次提取
│   └── 3.4 类别标准化
├── 第4部分：特征工程
│   ├── 4.1 评分特征计算
│   ├── 4.2 类别编码
│   └── 4.3 衍生特征
├── 第5部分：预处理后可视化
├── 第6部分：数据输出与验证
└── 第7部分：数据划分
```

### 3.2 完整Python代码

```python
# ============================================================================
# DWTS 数据预处理完整代码
# MCM 2026 Problem C: Data With The Stars
# ============================================================================
# 版本: 1.0
# 日期: 2026-01-30
# 说明: 本代码实现DWTS数据集的完整预处理流程，包含可视化分析
# ============================================================================

# ============================================================================
# 第1部分：环境配置与数据加载
# ============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# ----- 设置中文显示（可选，根据系统配置调整） -----
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 150
plt.rcParams['savefig.dpi'] = 300

# ============================================================================
# ⚠️⚠️⚠️ 重要提示：请修改以下路径为您的本地路径 ⚠️⚠️⚠️
# ============================================================================
# 【数据输入路径】请将以下路径替换为您的本地CSV文件路径
# 例如 Windows: "C:/Users/YourName/Desktop/2026_MCM_Problem_C_Data.csv"
# 例如 Mac/Linux: "/home/username/data/2026_MCM_Problem_C_Data.csv"
INPUT_DATA_PATH = "./2026_MCM_Problem_C_Data.csv"

# 【输出目录路径】处理后的数据和图片将保存在此目录
# 请确保该目录存在，或代码会自动创建
OUTPUT_DIR = "./preprocessing_output"

# 【图片保存开关】设为True则保存图片，设为False则仅显示
SAVE_FIGURES = True
# ============================================================================

# 创建输出目录
output_path = Path(OUTPUT_DIR)
output_path.mkdir(parents=True, exist_ok=True)
(output_path / 'figures').mkdir(exist_ok=True)
(output_path / 'data').mkdir(exist_ok=True)

print("=" * 80)
print("DWTS 数据预处理程序启动")
print("=" * 80)
print(f"输入文件: {INPUT_DATA_PATH}")
print(f"输出目录: {OUTPUT_DIR}")
print("=" * 80)

# 加载数据
print("\n[1/7] 正在加载原始数据...")
df_raw = pd.read_csv(INPUT_DATA_PATH)
print(f"✓ 数据加载成功！维度: {df_raw.shape[0]}行 × {df_raw.shape[1]}列")

# ============================================================================
# 第2部分：数据探索性分析（EDA）可视化 - 预处理前
# ============================================================================

print("\n[2/7] 生成预处理前可视化分析...")

# 创建6x2的大图进行综合展示
fig, axes = plt.subplots(3, 2, figsize=(16, 15))
fig.suptitle('DWTS Dataset - Pre-processing Exploratory Analysis', fontsize=16, fontweight='bold')

# 2.1 各季节参赛人数分布
ax1 = axes[0, 0]
season_counts = df_raw['season'].value_counts().sort_index()
bars = ax1.bar(season_counts.index, season_counts.values, color='steelblue', edgecolor='black', alpha=0.7)
ax1.set_xlabel('Season', fontsize=11)
ax1.set_ylabel('Number of Contestants', fontsize=11)
ax1.set_title('(a) Contestants per Season', fontsize=12, fontweight='bold')
ax1.axhline(y=season_counts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {season_counts.mean():.1f}')
ax1.legend(loc='upper right')
# 标记特殊季节
for season in [1, 2, 28]:
    if season in season_counts.index:
        idx = list(season_counts.index).index(season)
        bars[idx].set_color('coral')

# 2.2 比赛结果分布
ax2 = axes[0, 1]
results_counts = df_raw['results'].value_counts()
# 提取主要类别
top_results = results_counts.head(10)
colors = ['gold' if '1st' in x else 'silver' if '2nd' in x else 'chocolate' if '3rd' in x else 'lightblue' 
          for x in top_results.index]
bars2 = ax2.barh(range(len(top_results)), top_results.values, color=colors, edgecolor='black', alpha=0.8)
ax2.set_yticks(range(len(top_results)))
ax2.set_yticklabels(top_results.index, fontsize=9)
ax2.set_xlabel('Count', fontsize=11)
ax2.set_title('(b) Competition Results Distribution (Top 10)', fontsize=12, fontweight='bold')
ax2.invert_yaxis()

# 2.3 年龄分布
ax3 = axes[1, 0]
ax3.hist(df_raw['celebrity_age_during_season'], bins=20, color='teal', edgecolor='black', alpha=0.7)
ax3.axvline(x=df_raw['celebrity_age_during_season'].median(), color='red', linestyle='--', linewidth=2, 
            label=f'Median: {df_raw["celebrity_age_during_season"].median():.0f}')
ax3.set_xlabel('Age', fontsize=11)
ax3.set_ylabel('Frequency', fontsize=11)
ax3.set_title('(c) Celebrity Age Distribution', fontsize=12, fontweight='bold')
ax3.legend()

# 2.4 行业分布（Top 10）
ax4 = axes[1, 1]
industry_counts = df_raw['celebrity_industry'].value_counts().head(10)
colors_ind = plt.cm.Paired(np.linspace(0, 1, len(industry_counts)))
ax4.pie(industry_counts.values, labels=industry_counts.index, autopct='%1.1f%%', 
        colors=colors_ind, startangle=90, explode=[0.05]*len(industry_counts))
ax4.set_title('(d) Celebrity Industry Distribution (Top 10)', fontsize=12, fontweight='bold')

# 2.5 第一周评分分布（评委1）
ax5 = axes[2, 0]
valid_scores = df_raw['week1_judge1_score'][(df_raw['week1_judge1_score'] > 0)]
ax5.hist(valid_scores, bins=15, color='purple', edgecolor='black', alpha=0.7)
ax5.axvline(x=valid_scores.mean(), color='red', linestyle='--', linewidth=2, 
            label=f'Mean: {valid_scores.mean():.2f}')
ax5.set_xlabel('Score', fontsize=11)
ax5.set_ylabel('Frequency', fontsize=11)
ax5.set_title('(e) Week 1 Judge 1 Score Distribution (Excluding 0)', fontsize=12, fontweight='bold')
ax5.legend()

# 2.6 缺失值热力图
ax6 = axes[2, 1]
# 选择评分列展示缺失情况
score_cols = [col for col in df_raw.columns if 'score' in col]
missing_matrix = df_raw[score_cols].isnull().sum().values.reshape(11, 4)  # 11周 x 4评委
im = ax6.imshow(missing_matrix, cmap='YlOrRd', aspect='auto')
ax6.set_xticks(range(4))
ax6.set_xticklabels(['Judge 1', 'Judge 2', 'Judge 3', 'Judge 4'], fontsize=9)
ax6.set_yticks(range(11))
ax6.set_yticklabels([f'Week {i+1}' for i in range(11)], fontsize=9)
ax6.set_title('(f) Missing Values Heatmap (Judge Scores)', fontsize=12, fontweight='bold')
cbar = plt.colorbar(im, ax=ax6)
cbar.set_label('Missing Count', fontsize=10)

plt.tight_layout(rect=[0, 0, 1, 0.96])

if SAVE_FIGURES:
    fig.savefig(output_path / 'figures' / '01_pre_processing_eda.png', bbox_inches='tight')
    print("  ✓ 保存图片: 01_pre_processing_eda.png")
plt.show()
plt.close()

# ============================================================================
# 第3部分：数据清洗
# ============================================================================

print("\n[3/7] 执行数据清洗...")

df = df_raw.copy()

# ----- 3.1 0分标记处理 -----
print("  [3.1] 处理0分标记（被淘汰者标记）...")

# 创建字典记录每个选手的最后有效参赛周
def get_last_valid_week(row):
    """
    获取选手最后一个有效参赛周
    逻辑：找到评分从非0变为0的临界点
    """
    for week in range(1, 12):
        col = f'week{week}_judge1_score'
        if col in row.index:
            score = row[col]
            if pd.isna(score) or score == 0:
                return week - 1
    return 11  # 完成所有周

df['last_valid_week'] = df.apply(get_last_valid_week, axis=1)
print(f"    ✓ 选手最后有效周分布: 最小={df['last_valid_week'].min()}, 最大={df['last_valid_week'].max()}")

# ----- 3.2 评委数量动态识别 -----
print("  [3.2] 动态识别各季节评委数量...")

def get_judge_count(season, week):
    """
    根据季节和周次返回评委数量
    基于历史数据分析结果
    """
    # Season 19-20, 23-24, 30-31 有4位评委
    four_judge_seasons = [19, 20, 23, 24, 30, 31]
    if season in four_judge_seasons:
        return 4
    return 3

# 为每条记录添加评委数量
df['judge_count'] = df['season'].apply(lambda x: get_judge_count(x, 1))
print(f"    ✓ 4评委季节数: {(df['judge_count'] == 4).sum()}, 3评委季节数: {(df['judge_count'] == 3).sum()}")

# ----- 3.3 淘汰周次提取 -----
print("  [3.3] 从results字段提取淘汰周次...")

def extract_elimination_week(result):
    """
    从results字段提取淘汰周次
    返回值：
    - 正整数：被淘汰的周次
    - 0：冠军/亚军/季军等进入决赛的选手
    - -1：退赛
    """
    if pd.isna(result):
        return np.nan
    result = str(result)
    
    # 名次类结果
    if '1st Place' in result:
        return 0
    elif '2nd Place' in result:
        return 0
    elif '3rd Place' in result:
        return 0
    elif '4th Place' in result:
        return 0
    elif '5th Place' in result:
        return 0
    elif 'Withdrew' in result:
        return -1
    elif 'Eliminated Week' in result:
        # 提取周次数字
        import re
        match = re.search(r'Eliminated Week (\d+)', result)
        if match:
            return int(match.group(1))
    return np.nan

df['elimination_week'] = df['results'].apply(extract_elimination_week)
print(f"    ✓ 淘汰周次分布:")
print(f"       决赛圈(0): {(df['elimination_week'] == 0).sum()}人")
print(f"       退赛(-1): {(df['elimination_week'] == -1).sum()}人")
print(f"       周次淘汰: {(df['elimination_week'] > 0).sum()}人")

# ----- 3.4 类别标准化 -----
print("  [3.4] 类别字段标准化...")

# 行业字段统一大小写
df['celebrity_industry'] = df['celebrity_industry'].str.strip().str.title()
# 合并相似类别
industry_mapping = {
    'Social Media Personality': 'Social Media Personality',
    'Social media personality': 'Social Media Personality',
}
df['celebrity_industry'] = df['celebrity_industry'].replace(industry_mapping)
print(f"    ✓ 行业类别数: {df['celebrity_industry'].nunique()}")

# 国家字段标准化
df['is_usa'] = df['celebrity_homecountry/region'].apply(lambda x: 1 if x == 'United States' else 0)
print(f"    ✓ 美国选手: {df['is_usa'].sum()}人, 国际选手: {(df['is_usa'] == 0).sum()}人")

# ============================================================================
# 第4部分：特征工程
# ============================================================================

print("\n[4/7] 执行特征工程...")

# ----- 4.1 评分特征计算 -----
print("  [4.1] 计算评分相关特征...")

# 计算每周总分（动态考虑评委数量）
for week in range(1, 12):
    cols = [f'week{week}_judge{j}_score' for j in range(1, 5)]
    existing_cols = [c for c in cols if c in df.columns]
    
    # 计算总分（排除0和NaN）
    def calc_weekly_total(row):
        scores = []
        for col in existing_cols:
            score = row[col]
            if pd.notna(score) and score > 0:
                scores.append(score)
        return sum(scores) if scores else np.nan
    
    df[f'week{week}_total'] = df.apply(calc_weekly_total, axis=1)
    
    # 计算平均分
    df[f'week{week}_avg'] = df.apply(
        lambda row: np.mean([row[col] for col in existing_cols if pd.notna(row[col]) and row[col] > 0]) 
        if any(pd.notna(row[col]) and row[col] > 0 for col in existing_cols) else np.nan,
        axis=1
    )

# 计算选手整体表现特征
df['total_weeks_participated'] = df['last_valid_week']
df['avg_score_all_weeks'] = df[[f'week{w}_avg' for w in range(1, 12)]].mean(axis=1, skipna=True)
df['max_weekly_score'] = df[[f'week{w}_total' for w in range(1, 12)]].max(axis=1, skipna=True)
df['min_weekly_score'] = df[[f'week{w}_total' for w in range(1, 12)]].apply(
    lambda x: x[x > 0].min() if (x > 0).any() else np.nan, axis=1
)
df['score_improvement'] = df.apply(
    lambda row: row[f'week{row["last_valid_week"]}_avg'] - row['week1_avg'] 
    if row['last_valid_week'] > 1 and pd.notna(row.get(f'week{row["last_valid_week"]}_avg')) else np.nan,
    axis=1
)

print(f"    ✓ 新增特征: 每周总分/平均分(22列), 整体表现特征(5列)")

# ----- 4.2 类别编码 -----
print("  [4.2] 类别变量编码...")

# 舞伴ID编码（用于问题三分析）
partner_encoder = {name: idx for idx, name in enumerate(df['ballroom_partner'].unique())}
df['partner_id'] = df['ballroom_partner'].map(partner_encoder)
print(f"    ✓ 舞伴编码完成: {len(partner_encoder)}位舞伴")

# 行业独热编码（可选）
industry_dummies = pd.get_dummies(df['celebrity_industry'], prefix='ind')
df = pd.concat([df, industry_dummies], axis=1)
print(f"    ✓ 行业独热编码: {len(industry_dummies.columns)}个类别")

# ----- 4.3 规则阶段标记 -----
print("  [4.3] 添加投票规则阶段标记...")

def get_voting_rule_phase(season):
    """
    根据季节返回投票规则阶段
    Phase 1: 排名制（Season 1-2）
    Phase 2: 百分比制（Season 3-27）
    Phase 3: 排名制+评委拯救（Season 28-34）
    """
    if season <= 2:
        return 1
    elif season <= 27:
        return 2
    else:
        return 3

df['voting_rule_phase'] = df['season'].apply(get_voting_rule_phase)
print(f"    ✓ Phase 1 (排名制S1-2): {(df['voting_rule_phase'] == 1).sum()}人")
print(f"    ✓ Phase 2 (百分比制S3-27): {(df['voting_rule_phase'] == 2).sum()}人")
print(f"    ✓ Phase 3 (排名制+拯救S28-34): {(df['voting_rule_phase'] == 3).sum()}人")

# ============================================================================
# 第5部分：预处理后可视化
# ============================================================================

print("\n[5/7] 生成预处理后可视化分析...")

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
fig.suptitle('DWTS Dataset - Post-processing Analysis', fontsize=16, fontweight='bold')

# 5.1 各阶段选手分布
ax1 = axes[0, 0]
phase_counts = df['voting_rule_phase'].value_counts().sort_index()
colors_phase = ['#FF6B6B', '#4ECDC4', '#45B7D1']
bars = ax1.bar(['Phase 1\n(Rank S1-2)', 'Phase 2\n(Percent S3-27)', 'Phase 3\n(Rank+Save S28-34)'],
               phase_counts.values, color=colors_phase, edgecolor='black', alpha=0.8)
ax1.set_ylabel('Number of Contestants', fontsize=11)
ax1.set_title('(a) Contestants by Voting Rule Phase', fontsize=12, fontweight='bold')
for bar, count in zip(bars, phase_counts.values):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, str(count), 
             ha='center', va='bottom', fontsize=11, fontweight='bold')

# 5.2 评委数量分布
ax2 = axes[0, 1]
judge_counts = df['judge_count'].value_counts().sort_index()
colors_judge = ['#95E1D3', '#F38181']
ax2.pie(judge_counts.values, labels=[f'{j} Judges' for j in judge_counts.index],
        autopct='%1.1f%%', colors=colors_judge, startangle=90, explode=[0.02, 0.02])
ax2.set_title('(b) Judge Count Distribution', fontsize=12, fontweight='bold')

# 5.3 平均分分布（按阶段）
ax3 = axes[0, 2]
for phase in [1, 2, 3]:
    phase_data = df[df['voting_rule_phase'] == phase]['avg_score_all_weeks'].dropna()
    ax3.hist(phase_data, bins=15, alpha=0.5, label=f'Phase {phase}', edgecolor='black')
ax3.set_xlabel('Average Score', fontsize=11)
ax3.set_ylabel('Frequency', fontsize=11)
ax3.set_title('(c) Average Score Distribution by Phase', fontsize=12, fontweight='bold')
ax3.legend()

# 5.4 参赛周数分布
ax4 = axes[1, 0]
week_dist = df['total_weeks_participated'].value_counts().sort_index()
ax4.bar(week_dist.index, week_dist.values, color='mediumpurple', edgecolor='black', alpha=0.8)
ax4.set_xlabel('Weeks Participated', fontsize=11)
ax4.set_ylabel('Number of Contestants', fontsize=11)
ax4.set_title('(d) Distribution of Weeks Participated', fontsize=12, fontweight='bold')

# 5.5 舞伴出场次数（Top 15）
ax5 = axes[1, 1]
partner_counts = df['ballroom_partner'].value_counts().head(15)
colors_partner = plt.cm.viridis(np.linspace(0, 0.8, len(partner_counts)))
bars5 = ax5.barh(range(len(partner_counts)), partner_counts.values, color=colors_partner, edgecolor='black')
ax5.set_yticks(range(len(partner_counts)))
ax5.set_yticklabels(partner_counts.index, fontsize=9)
ax5.set_xlabel('Appearances', fontsize=11)
ax5.set_title('(e) Most Frequent Pro Partners (Top 15)', fontsize=12, fontweight='bold')
ax5.invert_yaxis()

# 5.6 年龄与最终排名关系
ax6 = axes[1, 2]
# 筛选前10名的数据
top10 = df[df['placement'] <= 10]
scatter = ax6.scatter(top10['celebrity_age_during_season'], top10['placement'], 
                      c=top10['voting_rule_phase'], cmap='coolwarm', alpha=0.6, s=50, edgecolor='black')
ax6.set_xlabel('Age', fontsize=11)
ax6.set_ylabel('Final Placement (1=Winner)', fontsize=11)
ax6.set_title('(f) Age vs Placement (Top 10 Finishers)', fontsize=12, fontweight='bold')
cbar = plt.colorbar(scatter, ax=ax6)
cbar.set_label('Voting Phase', fontsize=10)
ax6.invert_yaxis()  # 1在上面

plt.tight_layout(rect=[0, 0, 1, 0.96])

if SAVE_FIGURES:
    fig.savefig(output_path / 'figures' / '02_post_processing_analysis.png', bbox_inches='tight')
    print("  ✓ 保存图片: 02_post_processing_analysis.png")
plt.show()
plt.close()

# 5.7 特征相关性热力图（单独生成）
print("  生成特征相关性热力图...")
fig, ax = plt.subplots(figsize=(12, 10))

# 选择关键数值特征
corr_features = ['celebrity_age_during_season', 'placement', 'total_weeks_participated',
                 'avg_score_all_weeks', 'score_improvement', 'is_usa', 'voting_rule_phase',
                 'week1_avg', 'week1_total']
corr_features = [f for f in corr_features if f in df.columns]
corr_matrix = df[corr_features].corr()

mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',
            center=0, square=True, linewidths=0.5, ax=ax,
            annot_kws={'size': 9})
ax.set_title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')

plt.tight_layout()
if SAVE_FIGURES:
    fig.savefig(output_path / 'figures' / '03_correlation_heatmap.png', bbox_inches='tight')
    print("  ✓ 保存图片: 03_correlation_heatmap.png")
plt.show()
plt.close()

# ============================================================================
# 第6部分：数据输出与验证
# ============================================================================

print("\n[6/7] 输出处理后的数据...")

# ----- 6.1 输出完整处理后数据 -----
# CSV格式（推荐）
output_csv_path = output_path / 'data' / 'dwts_preprocessed_full.csv'
df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')
print(f"  ✓ 保存CSV: {output_csv_path}")
print(f"    文件大小: {output_csv_path.stat().st_size / 1024:.2f} KB")

# ----- 6.2 输出模型专用数据集 -----

# 问题一专用：周维度数据（长格式）
print("\n  生成问题一专用数据（周维度）...")
weekly_records = []
for idx, row in df.iterrows():
    for week in range(1, row['last_valid_week'] + 1):
        record = {
            'celebrity_name': row['celebrity_name'],
            'season': row['season'],
            'week': week,
            'voting_rule_phase': row['voting_rule_phase'],
            'judge_count': row['judge_count'],
            'weekly_total': row.get(f'week{week}_total', np.nan),
            'weekly_avg': row.get(f'week{week}_avg', np.nan),
            'is_eliminated_this_week': 1 if row['elimination_week'] == week else 0,
            'placement': row['placement']
        }
        weekly_records.append(record)

df_weekly = pd.DataFrame(weekly_records)
weekly_csv_path = output_path / 'data' / 'dwts_weekly_format.csv'
df_weekly.to_csv(weekly_csv_path, index=False)
print(f"  ✓ 保存周维度数据: {weekly_csv_path}")
print(f"    维度: {df_weekly.shape[0]}行 × {df_weekly.shape[1]}列")

# 问题三专用：选手特征数据
print("\n  生成问题三专用数据（选手特征）...")
feature_cols = ['celebrity_name', 'ballroom_partner', 'partner_id', 'celebrity_industry',
                'celebrity_age_during_season', 'is_usa', 'season', 'voting_rule_phase',
                'placement', 'total_weeks_participated', 'avg_score_all_weeks', 
                'score_improvement', 'elimination_week']
df_features = df[feature_cols].copy()
features_csv_path = output_path / 'data' / 'dwts_contestant_features.csv'
df_features.to_csv(features_csv_path, index=False)
print(f"  ✓ 保存选手特征数据: {features_csv_path}")

# ----- 6.3 展示处理后数据样例 -----
print("\n" + "=" * 80)
print("【处理后数据预览】")
print("=" * 80)

print("\n▶ 前10行数据:")
preview_cols = ['celebrity_name', 'season', 'placement', 'voting_rule_phase', 
                'judge_count', 'avg_score_all_weeks', 'total_weeks_participated']
print(df[preview_cols].head(10).to_string(index=False))

print("\n▶ 后5行数据:")
print(df[preview_cols].tail(5).to_string(index=False))

# ----- 6.4 数据完整性验证 -----
print("\n" + "=" * 80)
print("【数据完整性验证】")
print("=" * 80)

# 验证1：记录数
print(f"\n✓ 总记录数: {len(df)} (原始: {len(df_raw)})")

# 验证2：关键字段完整性
key_fields = ['celebrity_name', 'season', 'placement', 'voting_rule_phase', 'elimination_week']
for field in key_fields:
    null_count = df[field].isnull().sum()
    print(f"  {field}: {'✓ 完整' if null_count == 0 else f'⚠ 缺失{null_count}条'}")

# 验证3：数值范围检查
print(f"\n✓ 数值范围验证:")
print(f"  season: {df['season'].min()} - {df['season'].max()} (预期1-34)")
print(f"  placement: {df['placement'].min()} - {df['placement'].max()}")
print(f"  age: {df['celebrity_age_during_season'].min()} - {df['celebrity_age_during_season'].max()}")

# 验证4：分组完整性
print(f"\n✓ 各阶段选手总数: {df.groupby('voting_rule_phase').size().sum()} (应等于{len(df)})")

# ============================================================================
# 第7部分：数据划分（可选）
# ============================================================================

print("\n[7/7] 数据集划分...")

# 对于DWTS数据，由于问题本质是逆向推断而非传统预测，
# 数据划分主要用于交叉验证模型鲁棒性

# 方法1：按季节划分（推荐用于规则一致性分析）
print("\n  方法1: 按投票规则阶段划分")
train_seasons = list(range(3, 25))  # S3-S24 作为训练集（百分比制主体）
val_seasons = list(range(25, 28))    # S25-S27 作为验证集（百分比制后期）
test_seasons = list(range(28, 35))   # S28-S34 作为测试集（新规则）

df_train_s = df[df['season'].isin(train_seasons)]
df_val_s = df[df['season'].isin(val_seasons)]
df_test_s = df[df['season'].isin(test_seasons)]

print(f"    训练集 (S3-S24): {len(df_train_s)}人 ({len(df_train_s)/len(df)*100:.1f}%)")
print(f"    验证集 (S25-S27): {len(df_val_s)}人 ({len(df_val_s)/len(df)*100:.1f}%)")
print(f"    测试集 (S28-S34): {len(df_test_s)}人 ({len(df_test_s)/len(df)*100:.1f}%)")

# 保存划分后的数据
df_train_s.to_csv(output_path / 'data' / 'dwts_train_by_season.csv', index=False)
df_val_s.to_csv(output_path / 'data' / 'dwts_val_by_season.csv', index=False)
df_test_s.to_csv(output_path / 'data' / 'dwts_test_by_season.csv', index=False)
print("    ✓ 已保存按季节划分的数据集")

# 方法2：随机划分（用于特征影响分析）
print("\n  方法2: 随机划分 (7:2:1)")
from sklearn.model_selection import train_test_split

df_temp, df_test_r = train_test_split(df, test_size=0.1, random_state=42, stratify=df['voting_rule_phase'])
df_train_r, df_val_r = train_test_split(df_temp, test_size=0.222, random_state=42, stratify=df_temp['voting_rule_phase'])

print(f"    训练集: {len(df_train_r)}人 ({len(df_train_r)/len(df)*100:.1f}%)")
print(f"    验证集: {len(df_val_r)}人 ({len(df_val_r)/len(df)*100:.1f}%)")
print(f"    测试集: {len(df_test_r)}人 ({len(df_test_r)/len(df)*100:.1f}%)")

df_train_r.to_csv(output_path / 'data' / 'dwts_train_random.csv', index=False)
df_val_r.to_csv(output_path / 'data' / 'dwts_val_random.csv', index=False)
df_test_r.to_csv(output_path / 'data' / 'dwts_test_random.csv', index=False)
print("    ✓ 已保存随机划分的数据集")

# ============================================================================
# 处理完成总结
# ============================================================================

print("\n" + "=" * 80)
print("【数据预处理完成】")
print("=" * 80)

print(f"""
输出文件清单:
─────────────────────────────────────────────────────────────────
| 文件名                           | 格式  | 用途                    |
├─────────────────────────────────────────────────────────────────┤
| dwts_preprocessed_full.csv      | CSV  | 完整预处理数据          |
| dwts_weekly_format.csv          | CSV  | 问题一专用（周维度）    |
| dwts_contestant_features.csv    | CSV  | 问题三专用（选手特征）  |
| dwts_train_by_season.csv        | CSV  | 按季节划分-训练集       |
| dwts_val_by_season.csv          | CSV  | 按季节划分-验证集       |
| dwts_test_by_season.csv         | CSV  | 按季节划分-测试集       |
| dwts_train_random.csv           | CSV  | 随机划分-训练集         |
| dwts_val_random.csv             | CSV  | 随机划分-验证集         |
| dwts_test_random.csv            | CSV  | 随机划分-测试集         |
─────────────────────────────────────────────────────────────────

可视化图片:
─────────────────────────────────────────────────────────────────
| 01_pre_processing_eda.png       | 预处理前EDA分析          |
| 02_post_processing_analysis.png | 预处理后特征分析         |
| 03_correlation_heatmap.png      | 特征相关性热力图         |
─────────────────────────────────────────────────────────────────

数据验证方法:
1. 检查CSV文件首尾行是否与上述预览一致
2. 验证总行数: 应为421行
3. 检查voting_rule_phase列: 应只有1,2,3三个值
4. 检查placement列: 应在1-16范围内
5. 运行: pd.read_csv('dwts_preprocessed_full.csv').info()

所有文件保存在: {OUTPUT_DIR}
""")

print("=" * 80)
print("预处理脚本执行完毕！")
print("=" * 80)
```

---

## 四、数据补充说明

### 4.1 数据充分性评估

| 评估维度 | 现状 | 结论 |
|----------|------|------|
| **样本量** | 421人×34季 | ✅ 充足，满足统计推断要求 |
| **特征完整性** | 53个原始字段 | ✅ 核心特征齐全 |
| **时间跨度** | 2005-2024（约19年） | ✅ 覆盖多次规则变更 |
| **标签信息** | 淘汰结果明确 | ✅ 可构建约束条件 |

### 4.2 必须补充的信息

**无需外部数据补充**。题目提供的数据集已包含所有必要信息：
- 评委评分 → 计算J占比
- 淘汰结果 → 构建约束条件
- 选手特征 → 影响因素分析

### 4.3 可选补充（提升分析深度）

| 补充数据 | 用途 | 获取途径 | 美赛合理性 |
|----------|------|----------|------------|
| 舞蹈类型 | 周维度分析 | Wikipedia | ⭐⭐ 可选 |
| 收视率数据 | 观众基数估计 | Nielsen公开报告 | ⭐⭐ 可选 |
| 社交媒体粉丝数 | 粉丝基础先验 | 合理假设 | ⭐ 非必须 |

**重要**：美赛评审看重**已有数据的深度挖掘**，而非外部数据堆砌。

---

## 五、输出文件说明

### 5.1 文件清单

| 文件名 | 格式 | 大小（估计） | 用途 |
|--------|------|--------------|------|
| `dwts_preprocessed_full.csv` | CSV | ~150KB | 完整预处理数据 |
| `dwts_weekly_format.csv` | CSV | ~200KB | 问题一专用（周维度） |
| `dwts_contestant_features.csv` | CSV | ~50KB | 问题三专用（选手特征） |
| `dwts_train_by_season.csv` | CSV | ~100KB | 按季节划分-训练集 |
| `dwts_val_by_season.csv` | CSV | ~20KB | 按季节划分-验证集 |
| `dwts_test_by_season.csv` | CSV | ~30KB | 按季节划分-测试集 |

### 5.2 Python加载方法

```python
import pandas as pd

# 加载完整数据
df = pd.read_csv('./preprocessing_output/data/dwts_preprocessed_full.csv')

# 加载周维度数据（问题一）
df_weekly = pd.read_csv('./preprocessing_output/data/dwts_weekly_format.csv')

# 加载选手特征数据（问题三）
df_features = pd.read_csv('./preprocessing_output/data/dwts_contestant_features.csv')
```

### 5.3 Matlab加载方法

```matlab
% 加载CSV文件
data = readtable('./preprocessing_output/data/dwts_preprocessed_full.csv');

% 转换为数值矩阵（数值列）
numeric_cols = {'season', 'placement', 'celebrity_age_during_season', ...
                'voting_rule_phase', 'avg_score_all_weeks'};
data_matrix = table2array(data(:, numeric_cols));

% 保存为MAT格式（可选）
save('dwts_preprocessed.mat', 'data');
```

---

## 六、数据预处理总结

### 6.1 处理流程回顾

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        DWTS 数据预处理流程图                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────┐                                                            │
│  │ 原始数据    │  2026_MCM_Problem_C_Data.csv                               │
│  │ 421行×53列  │  含评委评分、选手信息、比赛结果                             │
│  └──────┬──────┘                                                            │
│         │                                                                   │
│         ▼                                                                   │
│  ┌─────────────────────────────────────────────────────────┐                │
│  │                    数据清洗阶段                          │                │
│  │  • 0分标记识别 → 提取last_valid_week                    │                │
│  │  • 评委数量动态识别 → judge_count字段                   │                │
│  │  • 淘汰周次提取 → elimination_week字段                  │                │
│  │  • 类别标准化 → 行业名称统一                            │                │
│  └──────┬──────────────────────────────────────────────────┘                │
│         │                                                                   │
│         ▼                                                                   │
│  ┌─────────────────────────────────────────────────────────┐                │
│  │                    特征工程阶段                          │                │
│  │  • 评分特征: week{X}_total, week{X}_avg                 │                │
│  │  • 整体表现: avg_score_all_weeks, score_improvement     │                │
│  │  • 类别编码: partner_id, industry独热编码               │                │
│  │  • 规则标记: voting_rule_phase (1/2/3)                  │                │
│  └──────┬──────────────────────────────────────────────────┘                │
│         │                                                                   │
│         ▼                                                                   │
│  ┌─────────────┐     ┌─────────────┐     ┌─────────────┐                   │
│  │ 完整数据集  │     │ 周维度数据  │     │ 特征数据    │                   │
│  │ (通用)      │     │ (问题一)    │     │ (问题三)    │                   │
│  └──────┬──────┘     └──────┬──────┘     └──────┬──────┘                   │
│         │                   │                   │                           │
│         └───────────────────┼───────────────────┘                           │
│                             │                                               │
│                             ▼                                               │
│  ┌─────────────────────────────────────────────────────────┐                │
│  │                    数据划分阶段                          │                │
│  │  方法1: 按季节划分 (S3-24训练/S25-27验证/S28-34测试)    │                │
│  │  方法2: 随机划分 (7:2:1, 分层抽样)                      │                │
│  └─────────────────────────────────────────────────────────┘                │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 6.2 关键预处理决策

| 决策点 | 选择 | 理由 |
|--------|------|------|
| 0分处理 | 标记而非删除 | 0分是淘汰标记，有建模价值 |
| 评委数量 | 动态计算 | 保证评分量纲一致性 |
| 缺失值填充 | 不填充 | 缺失有明确含义（评委4不存在） |
| 类别编码 | 保留原始+独热编码 | 兼顾可解释性和建模需求 |
| 数据划分 | 双方案 | 季节划分用于规则分析，随机划分用于特征分析 |

### 6.3 预处理质量保证

- ✅ **数据完整性**：421条记录全部保留，无误删
- ✅ **特征一致性**：评分量纲统一（考虑评委数量差异）
- ✅ **逻辑正确性**：淘汰周次与0分标记一致
- ✅ **可追溯性**：所有处理步骤有详细注释

---

## 七、常见问题解答（FAQ）

### Q1: 为什么不用均值填充缺失值？
**A**: 本数据集的"缺失值"有明确含义——NaN表示该季无第4评委，0表示选手已淘汰。均值填充会破坏数据逻辑。

### Q2: 小数评分（如8.5）是否需要处理？
**A**: 不需要。小数评分是正常的评委打分（部分季节允许半分），应保留原值。

### Q3: 如何验证预处理结果正确？
**A**: 
1. 检查`last_valid_week`与`elimination_week`是否一致
2. 验证Phase 1/2/3选手数量比例合理
3. 抽查几个选手的评分序列是否符合"非0→0"模式

### Q4: Matlab用户如何使用预处理结果？
**A**: 直接用`readtable()`加载CSV文件即可，代码示例见5.3节。

---

## 八、问题一专用：逆向推断数据准备

> **核心理念**：问题一是典型的**逆向问题（Inverse Problem）**——类似于CT扫描从投影反推密度分布。我们需要将数据转换为适合约束优化/MCMC采样的格式。

### 8.1 单周数据结构（约束优化输入）

```python
# ============================================================================
# 问题一专用：生成逆向推断所需的周维度数据
# ============================================================================

def prepare_inverse_problem_data(df):
    """
    为每一周生成逆向推断所需的数据结构
    
    输出格式:
    {
        (season, week): {
            'contestants': [name1, name2, ...],       # 当周仍在比赛的选手
            'judge_scores': [J1, J2, ...],           # 评委原始总分
            'judge_pct': [Pct1, Pct2, ...],          # 评委占比（归一化）
            'eliminated_idx': idx,                    # 被淘汰者在列表中的索引
            'eliminated_name': name,                  # 被淘汰者姓名
            'voting_rule': 'rank' or 'percent',       # 当季投票规则
            'judge_count': 3 or 4,                    # 评委人数
            'n_contestants': N                        # 当周选手数量
        }
    }
    """
    week_data = {}
    
    for season in df['season'].unique():
        season_df = df[df['season'] == season].copy()
        voting_rule = 'rank' if season <= 2 or season >= 28 else 'percent'
        
        # 确定该季有多少周比赛
        max_week = season_df['last_valid_week'].max()
        
        for week in range(1, max_week + 1):
            # 筛选当周仍在比赛的选手（评分>0）
            week_col = f'week{week}_total'
            if week_col not in season_df.columns:
                continue
                
            active_mask = season_df[week_col] > 0
            active_df = season_df[active_mask]
            
            if len(active_df) < 2:  # 至少需要2人才能有淘汰
                continue
            
            # 获取评委分
            judge_scores = active_df[week_col].values
            judge_pct = judge_scores / judge_scores.sum()
            
            # 找出本周被淘汰者
            eliminated_mask = active_df['elimination_week'] == week
            eliminated_df = active_df[eliminated_mask]
            
            if len(eliminated_df) == 0:
                # 本周无人淘汰（可能是特殊周）
                eliminated_idx = None
                eliminated_name = None
            else:
                eliminated_name = eliminated_df['celebrity_name'].values[0]
                eliminated_idx = list(active_df['celebrity_name']).index(eliminated_name)
            
            week_data[(season, week)] = {
                'contestants': list(active_df['celebrity_name']),
                'judge_scores': list(judge_scores),
                'judge_pct': list(judge_pct),
                'eliminated_idx': eliminated_idx,
                'eliminated_name': eliminated_name,
                'voting_rule': voting_rule,
                'judge_count': active_df['judge_count'].iloc[0],
                'n_contestants': len(active_df)
            }
    
    return week_data

# 使用示例
# week_data = prepare_inverse_problem_data(df)
# print(week_data[(27, 5)])  # 查看第27季第5周的数据
```

### 8.2 狄利克雷先验准备（贝叶斯MCMC输入）

```python
def generate_dirichlet_samples(judge_pct, n_samples=50000):
    """
    基于狄利克雷分布生成粉丝投票先验样本
    
    参数:
        judge_pct: 评委占比列表 [Pct_1, Pct_2, ..., Pct_N]
        n_samples: 采样数量
    
    返回:
        samples: 形状为 (n_samples, N) 的粉丝投票样本矩阵
        
    先验假设:
        - 无信息先验: α = [1, 1, ..., 1] (均匀分布)
        - 弱信息先验: α = judge_pct * k (评委分引导)
        
    参考: 王俊哲思路 - 狄利克雷分布假设粉丝投票先验
    """
    import numpy as np
    N = len(judge_pct)
    
    # 方法1: 无信息先验（推荐用于问题一初始估计）
    alpha_uniform = np.ones(N)
    samples_uniform = np.random.dirichlet(alpha_uniform, n_samples)
    
    # 方法2: 弱信息先验（假设粉丝投票与评委分正相关）
    # k为浓度参数，k越大先验越强
    k = 5
    alpha_informed = np.array(judge_pct) * k + 1
    samples_informed = np.random.dirichlet(alpha_informed, n_samples)
    
    return {
        'uniform': samples_uniform,
        'informed': samples_informed,
        'alpha_uniform': alpha_uniform,
        'alpha_informed': alpha_informed
    }
```

### 8.3 约束验证函数（检验采样是否满足淘汰约束）

```python
def check_elimination_constraint(fan_votes, judge_pct, elim_idx, method='percent'):
    """
    检验给定的粉丝投票是否满足淘汰约束
    
    参数:
        fan_votes: 粉丝投票占比 [F_1, F_2, ..., F_N], sum=1
        judge_pct: 评委占比 [J_1, J_2, ..., J_N], sum=1
        elim_idx: 被淘汰者索引
        method: 'percent' (百分比制) 或 'rank' (排名制)
    
    返回:
        True: 满足约束（被淘汰者确实是最低分/最高排名和）
        False: 不满足约束
        
    参考: J老师思路 - 约束满足问题
    """
    import numpy as np
    
    if method == 'percent':
        # 百分比制: 总分 = 0.5*J + 0.5*F
        total_scores = 0.5 * np.array(judge_pct) + 0.5 * np.array(fan_votes)
        # 被淘汰者总分必须最低
        return total_scores[elim_idx] == np.min(total_scores)
    
    elif method == 'rank':
        # 排名制: 总排名 = Rank(J) + Rank(F)
        # 注意: 排名从1开始，分数越高排名越小
        judge_ranks = len(judge_pct) + 1 - np.argsort(np.argsort(judge_pct)) 
        fan_ranks = len(fan_votes) + 1 - np.argsort(np.argsort(fan_votes))
        total_ranks = judge_ranks + fan_ranks
        # 被淘汰者总排名必须最高（数值最大）
        return total_ranks[elim_idx] == np.max(total_ranks)
    
    return False
```

---

## 九、问题二专用：反事实推演数据结构

> **核心理念**：问题二需要进行"如果用另一种规则，结果会怎样？"的反事实推演。

### 9.1 反事实模拟器数据准备

```python
def prepare_counterfactual_data(week_data, estimated_fan_votes):
    """
    准备反事实推演所需的数据
    
    参数:
        week_data: 由prepare_inverse_problem_data生成的周数据
        estimated_fan_votes: 问题一估计出的粉丝投票 {(season, week): [F_1, ..., F_N]}
    
    返回:
        results: {
            (season, week): {
                'original_rule': 'percent' or 'rank',
                'original_eliminated': name,
                'counterfactual_eliminated': name,  # 如果用另一种规则
                'would_change': True/False,
                'judge_scores': [...],
                'fan_votes': [...]
            }
        }
    """
    results = {}
    
    for (season, week), data in week_data.items():
        if data['eliminated_idx'] is None:
            continue
            
        fan_votes = estimated_fan_votes.get((season, week))
        if fan_votes is None:
            continue
        
        original_rule = data['voting_rule']
        judge_pct = data['judge_pct']
        
        # 用原规则计算（验证）
        original_elim = simulate_elimination(judge_pct, fan_votes, original_rule)
        
        # 用另一种规则计算（反事实）
        counter_rule = 'rank' if original_rule == 'percent' else 'percent'
        counter_elim = simulate_elimination(judge_pct, fan_votes, counter_rule)
        
        results[(season, week)] = {
            'original_rule': original_rule,
            'original_eliminated': data['eliminated_name'],
            'counterfactual_eliminated': data['contestants'][counter_elim],
            'would_change': original_elim != counter_elim,
            'judge_scores': data['judge_scores'],
            'fan_votes': fan_votes
        }
    
    return results

def simulate_elimination(judge_pct, fan_votes, method):
    """模拟在指定规则下谁会被淘汰"""
    import numpy as np
    
    if method == 'percent':
        total_scores = 0.5 * np.array(judge_pct) + 0.5 * np.array(fan_votes)
        return np.argmin(total_scores)
    else:  # rank
        judge_ranks = len(judge_pct) + 1 - np.argsort(np.argsort(judge_pct))
        fan_ranks = len(fan_votes) + 1 - np.argsort(np.argsort(fan_votes))
        total_ranks = judge_ranks + fan_ranks
        return np.argmax(total_ranks)
```

### 9.2 争议案例专用数据提取

```python
def extract_controversy_cases(df):
    """
    提取题目中提到的争议案例数据
    
    争议案例:
    - Jerry Rice (S2): 排名制下被淘汰的运动员
    - Billy Ray Cyrus (S25): 百分比制下幸存的名人
    - Bristol Palin (S11): 政治争议
    - Bobby Bones (S27): 技术垫底但夺冠，最大争议
    """
    controversy_names = ['Jerry Rice', 'Billy Ray Cyrus', 'Bristol Palin', 'Bobby Bones']
    
    controversy_data = {}
    for name in controversy_names:
        match = df[df['celebrity_name'].str.contains(name, case=False, na=False)]
        if len(match) > 0:
            row = match.iloc[0]
            controversy_data[name] = {
                'season': row['season'],
                'placement': row['placement'],
                'voting_rule': row['voting_rule_phase'],
                'avg_score': row.get('avg_score_all_weeks', None),
                'total_weeks': row.get('total_weeks_participated', None),
                'row_data': row.to_dict()
            }
    
    return controversy_data

# 使用示例
# controversy = extract_controversy_cases(df)
# print(f"Bobby Bones (S27): 排名{controversy['Bobby Bones']['placement']}, "
#       f"平均分{controversy['Bobby Bones']['avg_score']:.2f}")
```

---

## 十、问题三专用：特征工程增强

### 10.1 SHAP可解释性分析的特征矩阵

```python
def prepare_shap_features(df):
    """
    准备用于XGBoost + SHAP分析的特征矩阵
    
    参考: 王俊哲思路 - 随机森林/XGBoost特征重要性分析
    """
    # 显性特征
    explicit_features = [
        'celebrity_age_during_season',  # 年龄
        'is_usa',                        # 是否美国选手
        'voting_rule_phase',             # 投票规则阶段
    ]
    
    # 隐性特征：舞伴效应（Derek Hough等自带流量）
    # 使用partner_id作为类别特征
    
    # 时间特征：赛季趋势
    df_features = df.copy()
    df_features['season_normalized'] = (df_features['season'] - 1) / 33  # 归一化到0-1
    
    # 行业特征：One-hot编码
    industry_cols = [col for col in df.columns if col.startswith('ind_')]
    
    feature_cols = explicit_features + ['season_normalized', 'partner_id'] + industry_cols
    
    X = df_features[feature_cols].copy()
    
    # 目标变量（两个）
    y_placement = df_features['placement']  # 用于预测最终排名
    y_weeks = df_features['total_weeks_participated']  # 用于预测存活周数
    
    return X, y_placement, y_weeks, feature_cols
```

---

## 十一、预处理质量检查清单

> **提交前必须检查**：

| 检查项 | 验证方法 | 预期结果 |
|--------|----------|----------|
| ✅ 总记录数 | `len(df)` | 421 |
| ✅ 季节范围 | `df['season'].unique()` | 1-34共34个 |
| ✅ 规则阶段 | `df['voting_rule_phase'].value_counts()` | 1/2/3三个值 |
| ✅ 评委数量 | `df['judge_count'].unique()` | 仅3和4 |
| ✅ 0分逻辑 | 抽查任意被淘汰选手 | 淘汰后评分全为0 |
| ✅ 周数一致 | `df['last_valid_week'] == df['elimination_week']` | 大部分一致（决赛圈除外） |

---

---

## 十二、问题四专用：NSGA-II多目标优化数据准备

> **核心理念**：问题四需要设计新投票系统，NSGA-II需要三个目标函数的计算基础：公平性、稳定性、娱乐性。

### 12.1 NSGA-II目标函数数据准备

```python
def prepare_nsga2_data(df, week_data, estimated_fan_votes):
    """
    准备NSGA-II多目标优化所需的数据结构
    
    参数:
        df: 预处理后的完整数据集
        week_data: 问题一准备的周数据
        estimated_fan_votes: 问题一估算的粉丝投票
    
    返回:
        nsga2_input: {
            'historical_simulations': [...],  # 历史模拟数据
            'objective_calculators': {...},    # 目标函数计算器
            'decision_bounds': {...}           # 决策变量边界
        }
    """
    
    # 1. 准备历史模拟数据（每周的评委分和估算粉丝投票）
    simulation_data = []
    
    for (season, week), data in week_data.items():
        if data['eliminated_idx'] is None:
            continue
        
        fan_votes = estimated_fan_votes.get((season, week))
        if fan_votes is None:
            continue
        
        simulation_data.append({
            'season': season,
            'week': week,
            'n_contestants': data['n_contestants'],
            'judge_scores': np.array(data['judge_scores']),
            'judge_pct': np.array(data['judge_pct']),
            'fan_votes': np.array(fan_votes),
            'actual_eliminated': data['eliminated_idx'],
            'contestants': data['contestants']
        })
    
    # 2. 定义目标函数计算器
    def calculate_fairness(weight_judge, simulation_data):
        """公平性: 技术与最终排名的相关性"""
        correlations = []
        for sim in simulation_data:
            J = sim['judge_pct']
            F = sim['fan_votes']
            total = weight_judge * J + (1 - weight_judge) * F
            
            # Spearman相关
            from scipy.stats import spearmanr
            corr, _ = spearmanr(J, total)
            correlations.append(corr)
        return np.mean(correlations)
    
    def calculate_stability(weight_judge, simulation_data, n_bootstrap=100):
        """稳定性: 淘汰结果对投票波动的鲁棒性"""
        stability_scores = []
        for sim in simulation_data:
            J = sim['judge_pct']
            F_base = sim['fan_votes']
            
            # Bootstrap模拟投票波动
            consistent_count = 0
            for _ in range(n_bootstrap):
                # 添加±5%的随机扰动
                F_perturbed = F_base + np.random.uniform(-0.05, 0.05, len(F_base))
                F_perturbed = np.maximum(0, F_perturbed)
                F_perturbed = F_perturbed / F_perturbed.sum()
                
                # 计算原始和扰动后的淘汰结果
                total_base = weight_judge * J + (1 - weight_judge) * F_base
                total_pert = weight_judge * J + (1 - weight_judge) * F_perturbed
                
                elim_base = np.argmin(total_base)
                elim_pert = np.argmin(total_pert)
                
                if elim_base == elim_pert:
                    consistent_count += 1
            
            stability_scores.append(consistent_count / n_bootstrap)
        return np.mean(stability_scores)
    
    def calculate_entertainment(weight_judge, simulation_data):
        """娱乐性: 爆冷比例（高排名选手被淘汰）"""
        upsets = []
        for sim in simulation_data:
            J = sim['judge_pct']
            F = sim['fan_votes']
            total = weight_judge * J + (1 - weight_judge) * F
            
            eliminated_idx = np.argmin(total)
            judge_rank = len(J) - np.argsort(np.argsort(J))[eliminated_idx]  # 评委排名
            
            # 如果技术前50%的选手被淘汰，视为爆冷
            is_upset = judge_rank <= len(J) // 2
            upsets.append(is_upset)
        return np.mean(upsets)
    
    # 3. 决策变量边界
    decision_bounds = {
        'weight_judge': (0.0, 1.0),      # 评委权重
        'weight_fan': (0.0, 1.0),        # 粉丝权重（= 1 - weight_judge）
        'danger_zone_size': (2, 5),      # 危险区大小
        'judges_save_weight': (0.0, 0.5) # 评委拯救权重
    }
    
    return {
        'historical_simulations': simulation_data,
        'objective_calculators': {
            'fairness': calculate_fairness,
            'stability': calculate_stability,
            'entertainment': calculate_entertainment
        },
        'decision_bounds': decision_bounds,
        'n_simulations': len(simulation_data)
    }

# 使用示例
# nsga2_data = prepare_nsga2_data(df, week_data, estimated_fan_votes)
# fairness = nsga2_data['objective_calculators']['fairness'](0.5, nsga2_data['historical_simulations'])
```

### 12.2 系统评估特征矩阵

```python
def prepare_system_evaluation_matrix(df):
    """
    为各投票系统变体准备评估所需的特征矩阵
    
    用于：
    - 模拟不同投票系统在历史数据上的表现
    - 计算公平性/稳定性/娱乐性指标
    """
    
    # 按季节分组的汇总统计
    season_stats = df.groupby('season').agg({
        'placement': 'count',              # 每季选手数
        'avg_score_all_weeks': 'mean',     # 平均评分
        'total_weeks_participated': 'mean', # 平均存活周数
        'voting_rule_phase': 'first'       # 规则阶段
    }).reset_index()
    
    season_stats.columns = ['season', 'n_contestants', 'avg_score', 'avg_weeks', 'rule_phase']
    
    # 计算每个季节的评分差异度（标准差）
    score_variance = df.groupby('season')['avg_score_all_weeks'].std().reset_index()
    score_variance.columns = ['season', 'score_std']
    
    season_stats = season_stats.merge(score_variance, on='season')
    
    return season_stats

# 使用示例
# system_eval_data = prepare_system_evaluation_matrix(df)
```

---

## 十三、完整数据输出函数汇总

### 13.1 一键式数据准备函数

```python
def prepare_all_model_data(input_path, output_dir):
    """
    一键准备所有4个问题所需的数据
    
    ======================================================
    ⚠️ 请将下方路径修改为您的本地路径 ⚠️
    ======================================================
    
    参数:
        input_path: str, 原始数据文件路径
                    示例: './2026_MCM_Problem_C_Data.csv'
        output_dir: str, 输出目录路径
                    示例: './preprocessing_output'
    
    输出文件:
        {output_dir}/
        ├── data/
        │   ├── dwts_preprocessed_full.csv          # 完整预处理数据
        │   ├── dwts_weekly_format.csv              # 周维度数据
        │   ├── dwts_contestant_features.csv        # 选手特征数据
        │   ├── dwts_q1_inverse_problem.pkl         # 问题一专用数据
        │   ├── dwts_q2_counterfactual.pkl          # 问题二专用数据
        │   ├── dwts_q3_lmem_features.csv           # 问题三LMEM数据
        │   ├── dwts_q3_xgboost_features.csv        # 问题三XGBoost数据
        │   └── dwts_q4_nsga2_input.pkl             # 问题四专用数据
        └── figures/
            ├── eda_*.png                           # EDA可视化
            └── preprocessed_*.png                  # 预处理后可视化
    """
    import os
    import pickle
    from pathlib import Path
    
    # 创建输出目录
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    Path(f"{output_dir}/data").mkdir(exist_ok=True)
    Path(f"{output_dir}/figures").mkdir(exist_ok=True)
    
    print("="*60)
    print("DWTS 数据预处理管道启动")
    print("="*60)
    
    # Step 1: 加载原始数据
    print("\n[1/7] 加载原始数据...")
    df_raw = pd.read_csv(input_path)
    print(f"  ✓ 加载完成: {len(df_raw)}行 × {len(df_raw.columns)}列")
    
    # Step 2: 基础预处理
    print("\n[2/7] 执行基础预处理...")
    df = preprocess_dwts_data(df_raw)
    df.to_csv(f"{output_dir}/data/dwts_preprocessed_full.csv", index=False)
    print(f"  ✓ 保存: dwts_preprocessed_full.csv")
    
    # Step 3: 问题一数据准备
    print("\n[3/7] 准备问题一数据（逆向推断）...")
    week_data = prepare_inverse_problem_data(df)
    with open(f"{output_dir}/data/dwts_q1_inverse_problem.pkl", 'wb') as f:
        pickle.dump(week_data, f)
    print(f"  ✓ 保存: dwts_q1_inverse_problem.pkl ({len(week_data)}个周数据)")
    
    # Step 4: 问题三数据准备
    print("\n[4/7] 准备问题三数据（影响因素分析）...")
    
    # LMEM数据
    lmem_data = prepare_lmem_features(df)
    lmem_data.to_csv(f"{output_dir}/data/dwts_q3_lmem_features.csv", index=False)
    print(f"  ✓ 保存: dwts_q3_lmem_features.csv ({len(lmem_data.columns)}个特征)")
    
    # XGBoost+SHAP数据
    X, y_placement, y_weeks, feature_cols = prepare_shap_features(df)
    xgb_data = X.copy()
    xgb_data['y_placement'] = y_placement
    xgb_data['y_weeks'] = y_weeks
    xgb_data.to_csv(f"{output_dir}/data/dwts_q3_xgboost_features.csv", index=False)
    print(f"  ✓ 保存: dwts_q3_xgboost_features.csv ({len(feature_cols)}个特征)")
    
    # Step 5: 输出汇总
    print("\n[5/7] 生成数据汇总...")
    
    # 数据预览
    print("\n  >>> 预处理数据预览（前10行）:")
    print(df.head(10).to_string())
    print("\n  >>> 预处理数据预览（后5行）:")
    print(df.tail(5).to_string())
    
    # Step 6: 数据验证
    print("\n[6/7] 数据验证...")
    validation_results = validate_preprocessing(df)
    for check, result in validation_results.items():
        status = "✓" if result else "✗"
        print(f"  {status} {check}")
    
    # Step 7: 完成
    print("\n[7/7] 数据准备完成!")
    print("="*60)
    print(f"输出目录: {output_dir}")
    print("="*60)
    
    return {
        'df_preprocessed': df,
        'week_data': week_data,
        'lmem_data': lmem_data,
        'xgb_data': xgb_data
    }

def validate_preprocessing(df):
    """验证预处理结果的正确性"""
    results = {
        '总记录数=421': len(df) == 421,
        '季节范围1-34': df['season'].min() == 1 and df['season'].max() == 34,
        '规则阶段有效': set(df['voting_rule_phase'].unique()).issubset({1, 2, 3}),
        '评委数量有效': set(df['judge_count'].unique()).issubset({3, 4}),
        '无空值关键字段': not df[['season', 'celebrity_name', 'placement']].isna().any().any()
    }
    return results
```

### 13.2 LMEM特征准备函数

```python
def prepare_lmem_features(df):
    """
    准备问题三LMEM混合效应模型的特征矩阵
    
    固定效应：
    - Age (标准化)
    - Industry (One-Hot)
    - Gender (0/1)
    - Season_normalized
    
    随机效应：
    - Pro (舞伴ID)
    - Season (季数)
    
    响应变量：
    - avg_score_all_weeks (评委平均分)
    - total_weeks_participated (存活周数)
    """
    
    lmem_df = df.copy()
    
    # 固定效应特征
    # 年龄标准化
    lmem_df['age_standardized'] = (lmem_df['celebrity_age_during_season'] - 
                                    lmem_df['celebrity_age_during_season'].mean()) / \
                                   lmem_df['celebrity_age_during_season'].std()
    
    # 季节归一化
    lmem_df['season_normalized'] = (lmem_df['season'] - 1) / 33
    
    # 行业One-Hot（选择Top 10行业）
    top_industries = lmem_df['celebrity_industry'].value_counts().head(10).index.tolist()
    for ind in top_industries:
        col_name = f'ind_{ind.lower().replace(" ", "_").replace("/", "_")}'
        lmem_df[col_name] = (lmem_df['celebrity_industry'] == ind).astype(int)
    
    # 性别推断（简化：假设已有）
    # 如果没有性别字段，可以基于名字进行推断或标记为缺失
    if 'gender' not in lmem_df.columns:
        lmem_df['gender'] = 0  # 占位符
    
    # 随机效应变量
    # 舞伴ID编码
    lmem_df['pro_id'] = pd.Categorical(lmem_df['ballroom_partner']).codes
    
    # 响应变量
    lmem_df['y_avg_score'] = lmem_df['avg_score_all_weeks']
    lmem_df['y_weeks'] = lmem_df['total_weeks_participated']
    
    # 选择LMEM所需的列
    feature_cols = ['age_standardized', 'season_normalized', 'gender', 'pro_id', 'season',
                    'y_avg_score', 'y_weeks', 'celebrity_name', 'ballroom_partner']
    
    # 添加行业独热编码列
    industry_cols = [col for col in lmem_df.columns if col.startswith('ind_')]
    feature_cols.extend(industry_cols)
    
    return lmem_df[feature_cols]
```

---

## 十四、Kendall τ + Bootstrap 数据准备（问题二增强）

```python
def prepare_kendall_bootstrap_data(week_data, fan_vote_samples):
    """
    准备问题二Kendall τ系数 + Bootstrap敏感性分析的数据
    
    参数:
        week_data: 问题一准备的周数据字典
        fan_vote_samples: 贝叶斯MCMC的后验样本 {(season, week): samples[n_samples, n]}
    
    返回:
        bootstrap_input: {
            'week_keys': [...],               # 周标识
            'judge_pcts': [...],              # 评委占比
            'fan_vote_samples': [...],        # 粉丝投票样本
            'actual_eliminations': [...],     # 实际淘汰
            'voting_rules': [...]             # 投票规则
        }
    """
    
    bootstrap_input = {
        'week_keys': [],
        'judge_pcts': [],
        'fan_vote_samples': [],
        'actual_eliminations': [],
        'voting_rules': [],
        'n_contestants': []
    }
    
    for (season, week), data in week_data.items():
        if data['eliminated_idx'] is None:
            continue
        
        samples = fan_vote_samples.get((season, week))
        if samples is None:
            continue
        
        bootstrap_input['week_keys'].append((season, week))
        bootstrap_input['judge_pcts'].append(np.array(data['judge_pct']))
        bootstrap_input['fan_vote_samples'].append(samples)
        bootstrap_input['actual_eliminations'].append(data['eliminated_idx'])
        bootstrap_input['voting_rules'].append(data['voting_rule'])
        bootstrap_input['n_contestants'].append(data['n_contestants'])
    
    return bootstrap_input

def compute_kendall_tau_distribution(bootstrap_input, n_bootstrap=2000):
    """
    计算Kendall τ系数的Bootstrap分布
    
    返回:
        tau_results: {
            'tau_per_week': dict,              # 每周的τ估计
            'tau_global_mean': float,          # 全局τ均值
            'tau_global_ci': tuple,            # 95%置信区间
            'rule_difference_prob': float      # 规则导致淘汰差异的概率
        }
    """
    from scipy.stats import kendalltau
    
    tau_per_week = {}
    all_taus = []
    rule_differences = []
    
    for idx, key in enumerate(bootstrap_input['week_keys']):
        J_pct = bootstrap_input['judge_pcts'][idx]
        F_samples = bootstrap_input['fan_vote_samples'][idx]
        actual_elim = bootstrap_input['actual_eliminations'][idx]
        
        week_taus = []
        week_diffs = []
        
        # Bootstrap采样
        sample_indices = np.random.choice(len(F_samples), n_bootstrap, replace=True)
        
        for b in sample_indices:
            F = F_samples[b]
            
            # 两种合并方式
            # 排名制
            J_rank = len(J_pct) + 1 - np.argsort(np.argsort(J_pct))
            F_rank = len(F) + 1 - np.argsort(np.argsort(F))
            rank_total = J_rank + F_rank
            
            # 百分比制
            pct_total = 0.5 * J_pct + 0.5 * F
            
            # 计算τ
            tau, _ = kendalltau(rank_total, pct_total)
            week_taus.append(tau)
            
            # 检查是否导致不同淘汰
            elim_rank = np.argmax(rank_total)  # 排名制淘汰（总排名最高）
            elim_pct = np.argmin(pct_total)    # 百分比制淘汰（总分最低）
            week_diffs.append(elim_rank != elim_pct)
        
        tau_per_week[key] = {
            'mean': np.mean(week_taus),
            'std': np.std(week_taus),
            'ci_95': (np.percentile(week_taus, 2.5), np.percentile(week_taus, 97.5))
        }
        
        all_taus.extend(week_taus)
        rule_differences.extend(week_diffs)
    
    return {
        'tau_per_week': tau_per_week,
        'tau_global_mean': np.mean(all_taus),
        'tau_global_ci': (np.percentile(all_taus, 2.5), np.percentile(all_taus, 97.5)),
        'rule_difference_prob': np.mean(rule_differences)
    }
```

---

## 十五、数据预处理总结

### 15.1 各模型数据输出对照表

| 模型 | 输出文件 | 数据格式 | 关键字段 |
|------|----------|----------|----------|
| **问题一-约束优化** | `dwts_q1_inverse_problem.pkl` | Python dict | judge_pct, eliminated_idx, voting_rule |
| **问题一-贝叶斯MCMC** | `dwts_q1_inverse_problem.pkl` | Python dict | 同上 + 狄利克雷参数 |
| **问题二-Kendall τ** | 运行时计算 | - | tau_per_week, tau_global |
| **问题二-Bootstrap** | `dwts_q2_bootstrap_input.pkl` | Python dict | fan_vote_samples |
| **问题三-LMEM** | `dwts_q3_lmem_features.csv` | CSV | age_standardized, pro_id, y_avg_score |
| **问题三-XGBoost** | `dwts_q3_xgboost_features.csv` | CSV | 所有特征 + y_placement |
| **问题四-NSGA-II** | `dwts_q4_nsga2_input.pkl` | Python dict | objective_calculators, decision_bounds |

### 15.2 数据流向图

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        DWTS 数据预处理 → 模型输入 流向图                           │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  原始数据                                                                         │
│  2026_MCM_Problem_C_Data.csv                                                    │
│         │                                                                       │
│         ▼                                                                       │
│  ┌─────────────────────────────────────┐                                        │
│  │         基础预处理                    │                                        │
│  │  • 0分标记处理                        │                                        │
│  │  • 评委数量识别                       │                                        │
│  │  • 淘汰周次提取                       │                                        │
│  │  • 规则阶段标记                       │                                        │
│  └──────────────┬──────────────────────┘                                        │
│                 │                                                               │
│                 ▼                                                               │
│  ┌─────────────────────────────────────┐                                        │
│  │    dwts_preprocessed_full.csv       │                                        │
│  │    （预处理后完整数据）               │                                        │
│  └──────────────┬──────────────────────┘                                        │
│                 │                                                               │
│     ┌───────────┼───────────┬───────────┬───────────┐                           │
│     │           │           │           │           │                           │
│     ▼           ▼           ▼           ▼           ▼                           │
│  ┌──────┐   ┌──────┐   ┌──────┐   ┌──────┐   ┌──────┐                          │
│  │问题一│   │问题一│   │问题二│   │问题三│   │问题四│                          │
│  │约束  │   │贝叶斯│   │Kendall│  │LMEM/ │   │NSGA-II│                         │
│  │优化  │   │MCMC  │   │τ+Boot│  │XGBoost│  │      │                          │
│  └──┬───┘   └──┬───┘   └──┬───┘   └──┬───┘   └──┬───┘                          │
│     │          │          │          │          │                               │
│     ▼          ▼          ▼          ▼          ▼                               │
│  week_data  week_data  bootstrap  lmem_data  nsga2_data                         │
│     │          │       _input        │          │                               │
│     │          │          │          │          │                               │
│     └────┬─────┘          │          │          │                               │
│          │                │          │          │                               │
│          ▼                ▼          ▼          ▼                               │
│     V̂估算值  ───────►  反事实推演   特征重要性  帕累托前沿                       │
│     (粉丝投票)          τ分布       SHAP值    最优系统                           │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

**文档版本**: 3.0（模型驱动优化版）  
**更新日期**: 2026-01-30  
**核心改进**: 
- 添加投票规则三阶段详细说明
- 补充数据清洗雷区
- 新增问题一/二/三专用数据准备代码
- 增加狄利克雷先验和约束验证函数
- 优化反事实推演数据结构
- **新增**: 问题四NSGA-II多目标优化数据准备
- **新增**: Kendall τ + Bootstrap数据准备
- **新增**: LMEM特征矩阵生成函数
- **新增**: 一键式数据准备管道函数
- **新增**: 完整数据流向图

**适用题目**: MCM 2026 Problem C: Data With The Stars
