#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
DWTS æ•°æ®é¢„å¤„ç†å®Œæ•´å¯æ‰§è¡Œä»£ç 
MCM 2026 Problem C: Data With The Stars
================================================================================
ç‰ˆæœ¬: 3.0
æ—¥æœŸ: 2026-01-30
è¯´æ˜: æœ¬ä»£ç å®ç°DWTSæ•°æ®é›†çš„å®Œæ•´é¢„å¤„ç†æµç¨‹ï¼Œè¾“å‡ºæ‰€æœ‰æ¨¡å‹å»ºç«‹æ¨¡å—æ‰€éœ€çš„æ•°æ®

æ¨¡å‹æ•°æ®è¾“å‡º:
- é—®é¢˜ä¸€: çº¦æŸä¼˜åŒ–æ•°æ® + è´å¶æ–¯MCMCæ•°æ®
- é—®é¢˜äºŒ: Kendall Ï„ + Bootstrapæ•°æ®
- é—®é¢˜ä¸‰: LMEMç‰¹å¾çŸ©é˜µ + XGBoost-SHAPç‰¹å¾çŸ©é˜µ
- é—®é¢˜å››: NSGA-IIå¤šç›®æ ‡ä¼˜åŒ–æ•°æ®
================================================================================
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import json
import pickle
from typing import Dict, List, Tuple, Any, Optional
from scipy import stats
import re

warnings.filterwarnings('ignore')

# ============================================================================
# âš ï¸ é‡è¦ï¼šè¯·ä¿®æ”¹ä»¥ä¸‹è·¯å¾„ä¸ºæ‚¨çš„æœ¬åœ°æ•°æ®è·¯å¾„
# ============================================================================
# â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“ è¾“å…¥è·¯å¾„è®¾ç½® â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“
INPUT_DATA_PATH = "./2026_MCM_Problem_C_Data.csv"  # åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
# â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘ è¾“å…¥è·¯å¾„è®¾ç½® â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘

# â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“ è¾“å‡ºè·¯å¾„è®¾ç½® â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“
OUTPUT_DIR = "./preprocessing_output"              # è¾“å‡ºæ–‡ä»¶å¤¹
# â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘ è¾“å‡ºè·¯å¾„è®¾ç½® â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘
# ============================================================================


class DWTSDataPreprocessor:
    """
    DWTSæ•°æ®é¢„å¤„ç†ç±»
    
    åŠŸèƒ½:
    1. æ•°æ®åŠ è½½ä¸æ¸…æ´—
    2. ç‰¹å¾å·¥ç¨‹
    3. ä¸ºå„æ¨¡å‹ç”Ÿæˆä¸“ç”¨æ•°æ®
    """
    
    def __init__(self, input_path: str, output_dir: str):
        """
        åˆå§‹åŒ–é¢„å¤„ç†å™¨
        
        Args:
            input_path: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
        self.input_path = input_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / 'data').mkdir(exist_ok=True)
        (self.output_dir / 'figures').mkdir(exist_ok=True)
        (self.output_dir / 'models').mkdir(exist_ok=True)
        
        # æ•°æ®å­˜å‚¨
        self.df_raw = None
        self.df_clean = None
        
        # è¯„å§”æ•°é‡è§„åˆ™ï¼ˆæ ¹æ®å­£èŠ‚ï¼‰
        self.judge_count_rules = {
            range(1, 19): 3,    # S1-18: 3è¯„å§”
            range(19, 21): 4,   # S19-20: 4è¯„å§”
            range(21, 23): 3,   # S21-22: 3è¯„å§”
            range(23, 25): 4,   # S23-24: 4è¯„å§”
            range(25, 30): 3,   # S25-29: 3è¯„å§”
            range(30, 32): 4,   # S30-31: 4è¯„å§”
            range(32, 35): 3,   # S32-34: 3è¯„å§”
        }
        
    def get_judge_count(self, season: int) -> int:
        """æ ¹æ®å­£èŠ‚è·å–è¯„å§”æ•°é‡"""
        for season_range, count in self.judge_count_rules.items():
            if season in season_range:
                return count
        return 3  # é»˜è®¤3è¯„å§”
    
    def get_voting_rule(self, season: int) -> str:
        """
        æ ¹æ®å­£èŠ‚è·å–æŠ•ç¥¨è§„åˆ™
        
        Returns:
            'rank': æ’ååˆ¶ (S1-2, S28-34)
            'percent': ç™¾åˆ†æ¯”åˆ¶ (S3-27)
        """
        if season <= 2 or season >= 28:
            return 'rank'
        else:
            return 'percent'
    
    def get_voting_phase(self, season: int) -> int:
        """
        è·å–æŠ•ç¥¨è§„åˆ™é˜¶æ®µ
        
        Returns:
            1: æ’ååˆ¶ç¬¬ä¸€é˜¶æ®µ (S1-2)
            2: ç™¾åˆ†æ¯”åˆ¶ (S3-27)
            3: æ’ååˆ¶+è¯„å§”æ‹¯æ•‘ (S28-34)
        """
        if season <= 2:
            return 1
        elif season <= 27:
            return 2
        else:
            return 3
    
    # ========================================================================
    # ç¬¬1éƒ¨åˆ†ï¼šæ•°æ®åŠ è½½
    # ========================================================================
    
    def load_data(self) -> pd.DataFrame:
        """åŠ è½½åŸå§‹æ•°æ®"""
        print("=" * 60)
        print("ç¬¬1æ­¥ï¼šåŠ è½½åŸå§‹æ•°æ®")
        print("=" * 60)
        
        self.df_raw = pd.read_csv(self.input_path)
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ!")
        print(f"   - æ–‡ä»¶è·¯å¾„: {self.input_path}")
        print(f"   - æ•°æ®ç»´åº¦: {self.df_raw.shape[0]} è¡Œ Ã— {self.df_raw.shape[1]} åˆ—")
        print(f"   - å­£èŠ‚èŒƒå›´: S{self.df_raw['season'].min()} - S{self.df_raw['season'].max()}")
        
        return self.df_raw
    
    # ========================================================================
    # ç¬¬2éƒ¨åˆ†ï¼šæ•°æ®æ¸…æ´—
    # ========================================================================
    
    def clean_data(self) -> pd.DataFrame:
        """
        æ•°æ®æ¸…æ´—ä¸»æµç¨‹
        
        å¤„ç†å†…å®¹:
        1. 0åˆ†æ ‡è®°è¯†åˆ«
        2. è¯„å§”æ•°é‡åŠ¨æ€è¯†åˆ«
        3. æ·˜æ±°å‘¨æ¬¡æå–
        4. ç±»åˆ«æ ‡å‡†åŒ–
        """
        print("\n" + "=" * 60)
        print("ç¬¬2æ­¥ï¼šæ•°æ®æ¸…æ´—")
        print("=" * 60)
        
        df = self.df_raw.copy()
        
        # ----- 2.1 æ·»åŠ è¯„å§”æ•°é‡å­—æ®µ -----
        print("  [2.1] æ·»åŠ è¯„å§”æ•°é‡å­—æ®µ...")
        df['judge_count'] = df['season'].apply(self.get_judge_count)
        df['max_score'] = df['judge_count'] * 10  # æ»¡åˆ†
        
        # ----- 2.2 æ·»åŠ æŠ•ç¥¨è§„åˆ™å­—æ®µ -----
        print("  [2.2] æ·»åŠ æŠ•ç¥¨è§„åˆ™å­—æ®µ...")
        df['voting_rule'] = df['season'].apply(self.get_voting_rule)
        df['voting_phase'] = df['season'].apply(self.get_voting_phase)
        
        # ----- 2.3 æå–æ·˜æ±°å‘¨æ¬¡ -----
        print("  [2.3] æå–æ·˜æ±°å‘¨æ¬¡...")
        df['elimination_week'] = df['results'].apply(self._extract_elimination_week)
        
        # ----- 2.4 è®¡ç®—æ¯å‘¨æ€»åˆ†å’Œæœ€åæœ‰æ•ˆå‘¨ -----
        print("  [2.4] è®¡ç®—æ¯å‘¨æ€»åˆ†...")
        week_cols = []
        for week in range(1, 12):  # æœ€å¤š11å‘¨
            judge_cols = [f'week{week}_judge{j}_score' for j in range(1, 5)]
            existing_cols = [c for c in judge_cols if c in df.columns]
            
            if existing_cols:
                # è®¡ç®—æ¯å‘¨æ€»åˆ†ï¼ˆå¿½ç•¥NaNï¼‰
                total_col = f'week{week}_total'
                df[total_col] = df[existing_cols].sum(axis=1, skipna=True)
                week_cols.append(total_col)
                
                # è®¡ç®—æ¯å‘¨å¹³å‡åˆ†
                avg_col = f'week{week}_avg'
                df[avg_col] = df[existing_cols].mean(axis=1, skipna=True)
        
        # ----- 2.5 è®¡ç®—æœ€åæœ‰æ•ˆå‘¨ï¼ˆåˆ†æ•°>0çš„æœ€åä¸€å‘¨ï¼‰ -----
        print("  [2.5] è®¡ç®—æœ€åæœ‰æ•ˆå‘¨...")
        df['last_valid_week'] = df.apply(self._get_last_valid_week, axis=1, week_cols=week_cols)
        
        # ----- 2.6 ç±»åˆ«å­—æ®µæ ‡å‡†åŒ– -----
        print("  [2.6] ç±»åˆ«å­—æ®µæ ‡å‡†åŒ–...")
        if 'celebrity_industry' in df.columns:
            df['celebrity_industry'] = df['celebrity_industry'].str.strip().str.title()
        
        # ----- 2.7 èˆä¼´ç¼–ç  -----
        print("  [2.7] èˆä¼´ç¼–ç ...")
        df['partner_id'] = pd.factorize(df['ballroom_partner'])[0]
        
        # ----- 2.8 è®¡ç®—æ•´ä½“è¡¨ç°æŒ‡æ ‡ -----
        print("  [2.8] è®¡ç®—æ•´ä½“è¡¨ç°æŒ‡æ ‡...")
        df['avg_score_all_weeks'] = df[[c for c in week_cols if c in df.columns]].replace(0, np.nan).mean(axis=1)
        
        # è®¡ç®—åˆ†æ•°è¿›æ­¥è¶‹åŠ¿
        df['score_trend'] = df.apply(self._calculate_score_trend, axis=1, week_cols=week_cols)
        
        self.df_clean = df
        
        print(f"\nâœ… æ•°æ®æ¸…æ´—å®Œæˆ!")
        print(f"   - æ¸…æ´—åç»´åº¦: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
        print(f"   - æ–°å¢å­—æ®µ: judge_count, voting_rule, voting_phase, elimination_week, last_valid_week, partner_id, avg_score_all_weeks, score_trend")
        
        return df
    
    def _extract_elimination_week(self, result: str) -> int:
        """ä»resultså­—æ®µæå–æ·˜æ±°å‘¨æ¬¡"""
        if pd.isna(result):
            return -1
        
        result = str(result).lower()
        
        # å† å†›
        if 'winner' in result or '1st' in result:
            return 99  # ç‰¹æ®Šæ ‡è®°ï¼šå† å†›
        
        # Eliminated Week X
        match = re.search(r'week\s*(\d+)', result)
        if match:
            return int(match.group(1))
        
        # æ•°å­—å½¢å¼çš„åæ¬¡
        match = re.search(r'(\d+)(st|nd|rd|th)', result)
        if match:
            placement = int(match.group(1))
            # åæ¬¡è¶Šé å‰ï¼Œæ·˜æ±°å‘¨è¶Šæ™šï¼ˆç®€åŒ–ä¼°ç®—ï¼‰
            return 12 - placement if placement < 12 else 1
        
        return -1
    
    def _get_last_valid_week(self, row, week_cols: List[str]) -> int:
        """è·å–é€‰æ‰‹æœ€åä¸€ä¸ªæœ‰æ•ˆæ¯”èµ›å‘¨ï¼ˆåˆ†æ•°>0ï¼‰"""
        last_week = 0
        for i, col in enumerate(week_cols, 1):
            if col in row.index and row[col] > 0:
                last_week = i
        return last_week
    
    def _calculate_score_trend(self, row, week_cols: List[str]) -> float:
        """è®¡ç®—åˆ†æ•°å˜åŒ–è¶‹åŠ¿ï¼ˆæ–œç‡ï¼‰"""
        scores = []
        for col in week_cols:
            if col in row.index and row[col] > 0:
                scores.append(row[col])
        
        if len(scores) < 2:
            return 0.0
        
        # ç®€å•çº¿æ€§å›å½’æ–œç‡
        x = np.arange(len(scores))
        slope, _ = np.polyfit(x, scores, 1)
        return slope
    
    # ========================================================================
    # ç¬¬3éƒ¨åˆ†ï¼šé—®é¢˜ä¸€æ•°æ®å‡†å¤‡
    # ========================================================================
    
    def prepare_q1_constraint_optimization_data(self) -> Dict:
        """
        é—®é¢˜ä¸€æ–¹æ¡ˆä¸€ï¼šçº¦æŸä¼˜åŒ–æ¨¡å‹æ•°æ®å‡†å¤‡
        
        è¾“å‡ºç»“æ„:
        {
            (season, week): {
                'contestants': [name1, name2, ...],
                'judge_scores': [J1, J2, ...],
                'judge_pct': [pct1, pct2, ...],
                'judge_ranks': [R1, R2, ...],
                'eliminated_idx': idx,
                'eliminated_name': name,
                'voting_rule': 'rank' or 'percent',
                'n_contestants': N
            }
        }
        """
        print("\n" + "=" * 60)
        print("ç¬¬3æ­¥ï¼šå‡†å¤‡é—®é¢˜ä¸€ï¼ˆçº¦æŸä¼˜åŒ–ï¼‰æ•°æ®")
        print("=" * 60)
        
        df = self.df_clean
        week_data = {}
        
        for season in sorted(df['season'].unique()):
            season_df = df[df['season'] == season].copy()
            voting_rule = self.get_voting_rule(season)
            judge_count = self.get_judge_count(season)
            max_score = judge_count * 10
            
            # ç¡®å®šè¯¥å­£æœ‰å¤šå°‘å‘¨æ¯”èµ›
            max_week = int(season_df['last_valid_week'].max())
            
            for week in range(1, max_week + 1):
                week_col = f'week{week}_total'
                
                if week_col not in season_df.columns:
                    continue
                
                # ç­›é€‰å½“å‘¨ä»åœ¨æ¯”èµ›çš„é€‰æ‰‹ï¼ˆåˆ†æ•°>0ï¼‰
                active_mask = season_df[week_col] > 0
                active_df = season_df[active_mask].copy()
                
                if len(active_df) < 2:
                    continue
                
                # è·å–è¯„å§”åˆ†
                judge_scores = active_df[week_col].values.astype(float)
                contestants = active_df['celebrity_name'].tolist()
                
                # è®¡ç®—è¯„å§”åˆ†å æ¯”
                total_judge = judge_scores.sum()
                judge_pct = judge_scores / total_judge if total_judge > 0 else np.ones(len(judge_scores)) / len(judge_scores)
                
                # è®¡ç®—è¯„å§”æ’åï¼ˆåˆ†æ•°è¶Šé«˜æ’åè¶Šé å‰=æ•°å­—è¶Šå°ï¼‰
                judge_ranks = stats.rankdata(-judge_scores, method='min')
                
                # æ‰¾å‡ºè¢«æ·˜æ±°çš„é€‰æ‰‹
                eliminated_idx = None
                eliminated_name = None
                
                # è¢«æ·˜æ±°è€… = ä¸‹ä¸€å‘¨åˆ†æ•°å˜ä¸º0çš„é€‰æ‰‹
                if week < max_week:
                    next_week_col = f'week{week+1}_total'
                    if next_week_col in season_df.columns:
                        for i, (idx, row) in enumerate(active_df.iterrows()):
                            if season_df.loc[idx, next_week_col] == 0:
                                eliminated_idx = i
                                eliminated_name = row['celebrity_name']
                                break
                
                # å­˜å‚¨æ•°æ®
                week_data[(season, week)] = {
                    'contestants': contestants,
                    'judge_scores': [float(x) for x in judge_scores],
                    'judge_pct': [float(x) for x in judge_pct],
                    'judge_ranks': [int(x) for x in judge_ranks],
                    'eliminated_idx': int(eliminated_idx) if eliminated_idx is not None else None,
                    'eliminated_name': eliminated_name,
                    'voting_rule': voting_rule,
                    'judge_count': int(judge_count),
                    'max_score': int(max_score),
                    'n_contestants': len(contestants),
                    'season': int(season),
                    'week': int(week)
                }
        
        # ä¿å­˜æ•°æ®
        output_path = self.output_dir / 'models' / 'q1_constraint_optimization_data.json'
        
        # è½¬æ¢keyä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿JSONåºåˆ—åŒ–
        serializable_data = {f"{k[0]}_{k[1]}": v for k, v in week_data.items()}
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… é—®é¢˜ä¸€ï¼ˆçº¦æŸä¼˜åŒ–ï¼‰æ•°æ®å‡†å¤‡å®Œæˆ!")
        print(f"   - æ•°æ®æ¡æ•°: {len(week_data)} å‘¨")
        print(f"   - è¾“å‡ºè·¯å¾„: {output_path}")
        
        return week_data
    
    def prepare_q1_bayesian_mcmc_data(self) -> Dict:
        """
        é—®é¢˜ä¸€æ–¹æ¡ˆäºŒï¼šè´å¶æ–¯MCMC + ç‹„åˆ©å…‹é›·æ•°æ®å‡†å¤‡
        
        è¾“å‡ºç»“æ„:
        {
            (season, week): {
                'n': é€‰æ‰‹æ•°é‡,
                'alpha_prior': ç‹„åˆ©å…‹é›·å…ˆéªŒå‚æ•°,
                'judge_pct': è¯„å§”åˆ†å æ¯”,
                'constraint_type': 'rank' or 'percent',
                'eliminated_idx': è¢«æ·˜æ±°è€…ç´¢å¼•
            }
        }
        """
        print("\n" + "=" * 60)
        print("ç¬¬4æ­¥ï¼šå‡†å¤‡é—®é¢˜ä¸€ï¼ˆè´å¶æ–¯MCMCï¼‰æ•°æ®")
        print("=" * 60)
        
        # é¦–å…ˆè·å–çº¦æŸä¼˜åŒ–æ•°æ®
        constraint_data = self.prepare_q1_constraint_optimization_data() if not hasattr(self, '_q1_data') else self._q1_data
        
        mcmc_data = {}
        
        for key, data in constraint_data.items():
            n = data['n_contestants']
            judge_pct = np.array(data['judge_pct'])
            
            # è®¾è®¡ç‹„åˆ©å…‹é›·å…ˆéªŒå‚æ•°
            # æ–¹æ³•ï¼šä»¥è¯„å§”åˆ†å æ¯”ä¸ºåŸºç¡€ï¼ŒåŠ å…¥ä¸ç¡®å®šæ€§
            # alpha_i = base_alpha * (1 + judge_pct_i)
            base_alpha = 2.0  # å…ˆéªŒå¼ºåº¦
            alpha_prior = base_alpha * (1 + judge_pct * n)
            
            # ç¡®ä¿alpha > 0
            alpha_prior = np.maximum(alpha_prior, 0.5)
            
            mcmc_data[key] = {
                'n': int(n),
                'alpha_prior': [float(x) for x in alpha_prior],
                'judge_pct': data['judge_pct'],
                'judge_ranks': data['judge_ranks'],
                'constraint_type': data['voting_rule'],
                'eliminated_idx': data['eliminated_idx'],
                'contestants': data['contestants'],
                'season': int(data['season']),
                'week': int(data['week'])
            }
        
        # ä¿å­˜æ•°æ®
        output_path = self.output_dir / 'models' / 'q1_bayesian_mcmc_data.json'
        serializable_data = {f"{k[0]}_{k[1]}": v for k, v in mcmc_data.items()}
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… é—®é¢˜ä¸€ï¼ˆè´å¶æ–¯MCMCï¼‰æ•°æ®å‡†å¤‡å®Œæˆ!")
        print(f"   - æ•°æ®æ¡æ•°: {len(mcmc_data)} å‘¨")
        print(f"   - è¾“å‡ºè·¯å¾„: {output_path}")
        
        return mcmc_data
    
    # ========================================================================
    # ç¬¬4éƒ¨åˆ†ï¼šé—®é¢˜äºŒæ•°æ®å‡†å¤‡
    # ========================================================================
    
    def prepare_q2_kendall_bootstrap_data(self) -> Dict:
        """
        é—®é¢˜äºŒï¼šKendall Ï„ + Bootstrap æ•æ„Ÿæ€§åˆ†ææ•°æ®å‡†å¤‡
        
        è¾“å‡ºç»“æ„:
        {
            'rank_seasons': æ’ååˆ¶å­£èŠ‚æ•°æ®åˆ—è¡¨,
            'percent_seasons': ç™¾åˆ†æ¯”åˆ¶å­£èŠ‚æ•°æ®åˆ—è¡¨,
            'controversy_cases': äº‰è®®æ¡ˆä¾‹æ•°æ®,
            'cross_season_comparison': è·¨å­£èŠ‚å¯¹æ¯”æ•°æ®
        }
        """
        print("\n" + "=" * 60)
        print("ç¬¬5æ­¥ï¼šå‡†å¤‡é—®é¢˜äºŒï¼ˆKendall Ï„ + Bootstrapï¼‰æ•°æ®")
        print("=" * 60)
        
        df = self.df_clean
        
        # ----- åˆ†ç¦»æ’ååˆ¶å’Œç™¾åˆ†æ¯”åˆ¶æ•°æ® -----
        rank_seasons = df[df['voting_rule'] == 'rank'].copy()
        percent_seasons = df[df['voting_rule'] == 'percent'].copy()
        
        # ----- äº‰è®®æ¡ˆä¾‹æå– -----
        controversy_names = ['Jerry Rice', 'Billy Ray Cyrus', 'Bristol Palin', 'Bobby Bones']
        controversy_cases = df[df['celebrity_name'].isin(controversy_names)].copy()
        
        # ----- ä¸ºæ¯ä¸ªå­£èŠ‚è®¡ç®—Kendall Ï„æ‰€éœ€æ•°æ® -----
        season_rankings = []
        
        for season in sorted(df['season'].unique()):
            season_df = df[df['season'] == season].copy()
            
            # è®¡ç®—è¯„å§”åˆ†æ’å
            season_df['judge_rank'] = season_df['avg_score_all_weeks'].rank(ascending=False, method='min')
            
            # æœ€ç»ˆæ’å
            season_df['final_rank'] = season_df['placement']
            
            season_rankings.append({
                'season': int(season),
                'voting_rule': self.get_voting_rule(season),
                'voting_phase': self.get_voting_phase(season),
                'contestants': season_df['celebrity_name'].tolist(),
                'judge_ranks': season_df['judge_rank'].tolist(),
                'final_ranks': season_df['final_rank'].tolist(),
                'n_contestants': len(season_df)
            })
        
        # ----- Bootstrapæ•æ„Ÿæ€§åˆ†ææ‰€éœ€æ•°æ® -----
        bootstrap_data = {
            'all_seasons': season_rankings,
            'rank_rule_seasons': [s for s in season_rankings if s['voting_rule'] == 'rank'],
            'percent_rule_seasons': [s for s in season_rankings if s['voting_rule'] == 'percent']
        }
        
        # ----- æ±‡æ€»è¾“å‡ºæ•°æ® -----
        output_data = {
            'rank_seasons_summary': {
                'n_seasons': len(rank_seasons['season'].unique()),
                'seasons': sorted(rank_seasons['season'].unique().tolist()),
                'total_contestants': len(rank_seasons)
            },
            'percent_seasons_summary': {
                'n_seasons': len(percent_seasons['season'].unique()),
                'seasons': sorted(percent_seasons['season'].unique().tolist()),
                'total_contestants': len(percent_seasons)
            },
            'controversy_cases': controversy_cases[['celebrity_name', 'season', 'placement', 'voting_rule', 'avg_score_all_weeks']].to_dict('records'),
            'season_rankings': season_rankings,
            'bootstrap_data': bootstrap_data
        }
        
        # ä¿å­˜æ•°æ®
        output_path = self.output_dir / 'models' / 'q2_kendall_bootstrap_data.json'
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜è¯¦ç»†CSVæ•°æ®
        rank_seasons.to_csv(self.output_dir / 'data' / 'q2_rank_seasons.csv', index=False)
        percent_seasons.to_csv(self.output_dir / 'data' / 'q2_percent_seasons.csv', index=False)
        controversy_cases.to_csv(self.output_dir / 'data' / 'q2_controversy_cases.csv', index=False)
        
        print(f"âœ… é—®é¢˜äºŒï¼ˆKendall Ï„ + Bootstrapï¼‰æ•°æ®å‡†å¤‡å®Œæˆ!")
        print(f"   - æ’ååˆ¶å­£èŠ‚: {len(rank_seasons['season'].unique())} å­£")
        print(f"   - ç™¾åˆ†æ¯”åˆ¶å­£èŠ‚: {len(percent_seasons['season'].unique())} å­£")
        print(f"   - äº‰è®®æ¡ˆä¾‹: {len(controversy_cases)} æ¡")
        print(f"   - è¾“å‡ºè·¯å¾„: {output_path}")
        
        return output_data
    
    # ========================================================================
    # ç¬¬5éƒ¨åˆ†ï¼šé—®é¢˜ä¸‰æ•°æ®å‡†å¤‡
    # ========================================================================
    
    def prepare_q3_lmem_data(self) -> Tuple[pd.DataFrame, Dict]:
        """
        é—®é¢˜ä¸‰æ–¹æ¡ˆä¸€ï¼šçº¿æ€§æ··åˆæ•ˆåº”æ¨¡å‹(LMEM)æ•°æ®å‡†å¤‡
        
        è¾“å‡º:
        - ç‰¹å¾çŸ©é˜µ X (å›ºå®šæ•ˆåº” + éšæœºæ•ˆåº”æ ‡è¯†)
        - ç›®æ ‡å˜é‡ y (æœ€ç»ˆæ’å / å¹³å‡è¯„åˆ†)
        """
        print("\n" + "=" * 60)
        print("ç¬¬6æ­¥ï¼šå‡†å¤‡é—®é¢˜ä¸‰ï¼ˆLMEMï¼‰æ•°æ®")
        print("=" * 60)
        
        df = self.df_clean.copy()
        
        # ----- å›ºå®šæ•ˆåº”ç‰¹å¾ -----
        fixed_effects = pd.DataFrame()
        
        # å¹´é¾„ï¼ˆè¿ç»­å˜é‡ï¼Œæ ‡å‡†åŒ–ï¼‰
        fixed_effects['age'] = df['celebrity_age_during_season']
        fixed_effects['age_scaled'] = (fixed_effects['age'] - fixed_effects['age'].mean()) / fixed_effects['age'].std()
        
        # æ€§åˆ«ï¼ˆæ ¹æ®industryæ¨æ–­ï¼Œå¦‚æœ‰ä¸“é—¨å­—æ®µåˆ™ä½¿ç”¨ï¼‰
        # è¿™é‡Œä½¿ç”¨è¡Œä¸šä½œä¸ºä»£ç†
        
        # è¡Œä¸šï¼ˆç±»åˆ«å˜é‡ï¼Œç‹¬çƒ­ç¼–ç ï¼‰
        industry_dummies = pd.get_dummies(df['celebrity_industry'], prefix='industry')
        
        # å­£èŠ‚ï¼ˆæ§åˆ¶å˜é‡ï¼‰
        fixed_effects['season'] = df['season']
        fixed_effects['season_scaled'] = (df['season'] - df['season'].mean()) / df['season'].std()
        
        # æŠ•ç¥¨è§„åˆ™é˜¶æ®µ
        fixed_effects['voting_phase'] = df['voting_phase']
        
        # ----- éšæœºæ•ˆåº”æ ‡è¯† -----
        random_effects = pd.DataFrame()
        random_effects['partner_id'] = df['partner_id']
        random_effects['partner_name'] = df['ballroom_partner']
        random_effects['season_group'] = df['season']  # å­£èŠ‚ä½œä¸ºéšæœºæ•ˆåº”åˆ†ç»„
        
        # ----- ç›®æ ‡å˜é‡ -----
        targets = pd.DataFrame()
        targets['placement'] = df['placement']  # æœ€ç»ˆæ’å
        targets['avg_score'] = df['avg_score_all_weeks']  # å¹³å‡è¯„åˆ†
        targets['last_week'] = df['last_valid_week']  # å­˜æ´»å‘¨æ•°
        
        # ----- åˆå¹¶æ•°æ® -----
        lmem_data = pd.concat([
            df[['celebrity_name', 'season', 'ballroom_partner']],
            fixed_effects,
            random_effects,
            industry_dummies,
            targets
        ], axis=1)
        
        # ç§»é™¤ç¼ºå¤±å€¼
        lmem_data = lmem_data.dropna(subset=['avg_score', 'placement'])
        
        # ä¿å­˜æ•°æ®
        output_path = self.output_dir / 'data' / 'q3_lmem_features.csv'
        lmem_data.to_csv(output_path, index=False)
        
        # ä¿å­˜å…ƒä¿¡æ¯
        meta_info = {
            'fixed_effects': ['age_scaled', 'season_scaled', 'voting_phase'] + [c for c in industry_dummies.columns],
            'random_effects': ['partner_id', 'season_group'],
            'target_variables': ['placement', 'avg_score', 'last_week'],
            'n_samples': len(lmem_data),
            'n_partners': df['partner_id'].nunique(),
            'n_industries': df['celebrity_industry'].nunique()
        }
        
        with open(self.output_dir / 'models' / 'q3_lmem_meta.json', 'w') as f:
            json.dump(meta_info, f, indent=2)
        
        print(f"âœ… é—®é¢˜ä¸‰ï¼ˆLMEMï¼‰æ•°æ®å‡†å¤‡å®Œæˆ!")
        print(f"   - æ ·æœ¬æ•°: {len(lmem_data)}")
        print(f"   - å›ºå®šæ•ˆåº”æ•°: {len(meta_info['fixed_effects'])}")
        print(f"   - éšæœºæ•ˆåº”ç»„: partner_id ({meta_info['n_partners']}ä¸ª), season")
        print(f"   - è¾“å‡ºè·¯å¾„: {output_path}")
        
        return lmem_data, meta_info
    
    def prepare_q3_xgboost_shap_data(self) -> Tuple[pd.DataFrame, pd.Series, Dict]:
        """
        é—®é¢˜ä¸‰æ–¹æ¡ˆäºŒï¼šXGBoost + SHAP å¯è§£é‡Šæ€§åˆ†ææ•°æ®å‡†å¤‡
        
        è¾“å‡º:
        - X: ç‰¹å¾çŸ©é˜µ
        - y: ç›®æ ‡å˜é‡
        - feature_info: ç‰¹å¾è¯´æ˜
        """
        print("\n" + "=" * 60)
        print("ç¬¬7æ­¥ï¼šå‡†å¤‡é—®é¢˜ä¸‰ï¼ˆXGBoost + SHAPï¼‰æ•°æ®")
        print("=" * 60)
        
        df = self.df_clean.copy()
        
        # ----- æ„å»ºç‰¹å¾çŸ©é˜µ -----
        features = pd.DataFrame()
        
        # æ•°å€¼ç‰¹å¾
        features['age'] = df['celebrity_age_during_season']
        features['season'] = df['season']
        features['voting_phase'] = df['voting_phase']
        features['avg_score'] = df['avg_score_all_weeks']
        features['score_trend'] = df['score_trend']
        features['partner_id'] = df['partner_id']
        features['judge_count'] = df['judge_count']
        
        # ç±»åˆ«ç‰¹å¾ç¼–ç 
        # è¡Œä¸šç¼–ç 
        features['industry_encoded'] = pd.factorize(df['celebrity_industry'])[0]
        
        # å›½å®¶ç¼–ç 
        if 'celebrity_homecountry/region' in df.columns:
            features['is_usa'] = (df['celebrity_homecountry/region'] == 'United States').astype(int)
        
        # è®¡ç®—èˆä¼´å†å²èƒœç‡
        partner_win_rate = df.groupby('ballroom_partner').apply(
            lambda x: (x['placement'] <= 3).sum() / len(x)
        ).to_dict()
        features['partner_win_rate'] = df['ballroom_partner'].map(partner_win_rate)
        
        # è®¡ç®—åŒè¡Œä¸šå†å²è¡¨ç°
        industry_avg_placement = df.groupby('celebrity_industry')['placement'].mean().to_dict()
        features['industry_avg_placement'] = df['celebrity_industry'].map(industry_avg_placement)
        
        # ----- ç›®æ ‡å˜é‡ -----
        # äºŒåˆ†ç±»ï¼šæ˜¯å¦è¿›å…¥å‰3
        y_binary = (df['placement'] <= 3).astype(int)
        
        # å›å½’ï¼šæœ€ç»ˆæ’å
        y_regression = df['placement']
        
        # ç§»é™¤ç¼ºå¤±å€¼
        valid_mask = features.notna().all(axis=1)
        X = features[valid_mask].copy()
        y_bin = y_binary[valid_mask]
        y_reg = y_regression[valid_mask]
        
        # ç‰¹å¾è¯´æ˜
        feature_info = {
            'numerical_features': ['age', 'season', 'avg_score', 'score_trend', 'partner_win_rate', 'industry_avg_placement'],
            'categorical_features': ['voting_phase', 'partner_id', 'judge_count', 'industry_encoded', 'is_usa'],
            'target_binary': 'top3 (placement <= 3)',
            'target_regression': 'placement',
            'n_samples': len(X),
            'n_features': X.shape[1]
        }
        
        # ä¿å­˜æ•°æ®
        X.to_csv(self.output_dir / 'data' / 'q3_xgboost_features.csv', index=False)
        pd.DataFrame({'y_binary': y_bin, 'y_regression': y_reg}).to_csv(
            self.output_dir / 'data' / 'q3_xgboost_targets.csv', index=False
        )
        
        with open(self.output_dir / 'models' / 'q3_xgboost_meta.json', 'w') as f:
            json.dump(feature_info, f, indent=2)
        
        print(f"âœ… é—®é¢˜ä¸‰ï¼ˆXGBoost + SHAPï¼‰æ•°æ®å‡†å¤‡å®Œæˆ!")
        print(f"   - æ ·æœ¬æ•°: {len(X)}")
        print(f"   - ç‰¹å¾æ•°: {X.shape[1]}")
        print(f"   - æ•°å€¼ç‰¹å¾: {len(feature_info['numerical_features'])}")
        print(f"   - ç±»åˆ«ç‰¹å¾: {len(feature_info['categorical_features'])}")
        print(f"   - è¾“å‡ºè·¯å¾„: {self.output_dir / 'data' / 'q3_xgboost_features.csv'}")
        
        return X, y_reg, feature_info
    
    # ========================================================================
    # ç¬¬6éƒ¨åˆ†ï¼šé—®é¢˜å››æ•°æ®å‡†å¤‡
    # ========================================================================
    
    def prepare_q4_nsga2_data(self) -> Dict:
        """
        é—®é¢˜å››ï¼šNSGA-II + å¸•ç´¯æ‰˜å‰æ²¿ å¤šç›®æ ‡ä¼˜åŒ–æ•°æ®å‡†å¤‡
        
        ä¸‰ä¸ªä¼˜åŒ–ç›®æ ‡:
        1. å…¬å¹³æ€§ï¼ˆè¯„å§”åˆ†ä¸æœ€ç»ˆç»“æœçš„ç›¸å…³æ€§ï¼‰
        2. ç¨³å®šæ€§ï¼ˆç»“æœå¯¹æŠ•ç¥¨å™ªå£°çš„æ•æ„Ÿåº¦ï¼‰
        3. å¨±ä¹æ€§ï¼ˆæ‚¬å¿µç¨‹åº¦/å†·é—¨æ¦‚ç‡ï¼‰
        
        å†³ç­–å˜é‡:
        - w: è¯„å§”åˆ†æƒé‡ [0, 1]
        - threshold: æ·˜æ±°é˜ˆå€¼
        """
        print("\n" + "=" * 60)
        print("ç¬¬8æ­¥ï¼šå‡†å¤‡é—®é¢˜å››ï¼ˆNSGA-IIï¼‰æ•°æ®")
        print("=" * 60)
        
        df = self.df_clean.copy()
        
        # ----- å†å²æ•°æ®ç»Ÿè®¡ï¼ˆç”¨äºç›®æ ‡å‡½æ•°è®¡ç®—ï¼‰ -----
        
        # 1. è®¡ç®—å„å­£èŠ‚è¯„å§”åˆ†ä¸æœ€ç»ˆæ’åçš„ç›¸å…³æ€§
        season_correlations = []
        for season in df['season'].unique():
            season_df = df[df['season'] == season]
            if len(season_df) > 3:
                corr = season_df['avg_score_all_weeks'].corr(season_df['placement'])
                season_correlations.append({
                    'season': int(season),
                    'judge_placement_corr': corr,
                    'voting_rule': self.get_voting_rule(season)
                })
        
        # 2. è®¡ç®—"å†·é—¨"é¢‘ç‡ï¼ˆè¯„å§”åˆ†é«˜ä½†æ’åå·®çš„é€‰æ‰‹ï¼‰
        df['is_upset'] = ((df['avg_score_all_weeks'] > df['avg_score_all_weeks'].median()) & 
                         (df['placement'] > df['placement'].median())).astype(int)
        
        upset_rate_by_rule = df.groupby('voting_rule')['is_upset'].mean().to_dict()
        
        # 3. å‡†å¤‡æ¨¡æ‹Ÿå™¨æ‰€éœ€æ•°æ®
        simulation_data = []
        for season in sorted(df['season'].unique()):
            season_df = df[df['season'] == season].copy()
            
            simulation_data.append({
                'season': int(season),
                'voting_rule': self.get_voting_rule(season),
                'n_contestants': len(season_df),
                'judge_scores': season_df['avg_score_all_weeks'].tolist(),
                'final_placements': season_df['placement'].tolist(),
                'contestants': season_df['celebrity_name'].tolist()
            })
        
        # ----- å†³ç­–å˜é‡è¾¹ç•Œ -----
        decision_bounds = {
            'w_judge': [0.0, 1.0],      # è¯„å§”æƒé‡
            'w_fan': [0.0, 1.0],        # ç²‰ä¸æƒé‡ï¼ˆ= 1 - w_judgeï¼‰
            'threshold_low': [0.0, 0.3], # ä½åˆ†æ·˜æ±°é˜ˆå€¼
            'save_probability': [0.0, 0.5]  # è¯„å§”æ‹¯æ•‘æ¦‚ç‡
        }
        
        # ----- æ±‡æ€»è¾“å‡º -----
        nsga2_data = {
            'objectives': {
                'fairness': 'è¯„å§”åˆ†ä¸æœ€ç»ˆæ’åçš„Spearmanç›¸å…³ç³»æ•°',
                'stability': 'ç»“æœå¯¹æŠ•ç¥¨å™ªå£°çš„æ•æ„Ÿåº¦ï¼ˆæ ‡å‡†å·®ï¼‰',
                'entertainment': 'æ‚¬å¿µç¨‹åº¦ï¼ˆå†·é—¨æ¦‚ç‡ï¼‰'
            },
            'decision_variables': decision_bounds,
            'historical_data': {
                'season_correlations': season_correlations,
                'upset_rate_by_rule': upset_rate_by_rule,
                'simulation_data': simulation_data
            },
            'constraints': {
                'w_sum': 'w_judge + w_fan = 1',
                'elimination_rule': 'æ¯å‘¨è‡³å°‘æ·˜æ±°1äººï¼ˆé™¤å†³èµ›ï¼‰'
            }
        }
        
        # ä¿å­˜æ•°æ®
        output_path = self.output_dir / 'models' / 'q4_nsga2_data.json'
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(nsga2_data, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜è¯¦ç»†æ¨¡æ‹Ÿæ•°æ®
        pd.DataFrame(season_correlations).to_csv(
            self.output_dir / 'data' / 'q4_season_correlations.csv', index=False
        )
        
        print(f"âœ… é—®é¢˜å››ï¼ˆNSGA-IIï¼‰æ•°æ®å‡†å¤‡å®Œæˆ!")
        print(f"   - ä¼˜åŒ–ç›®æ ‡æ•°: 3 (å…¬å¹³æ€§/ç¨³å®šæ€§/å¨±ä¹æ€§)")
        print(f"   - å†³ç­–å˜é‡æ•°: 4")
        print(f"   - å†å²å­£èŠ‚æ•°: {len(simulation_data)}")
        print(f"   - è¾“å‡ºè·¯å¾„: {output_path}")
        
        return nsga2_data
    
    # ========================================================================
    # ç¬¬7éƒ¨åˆ†ï¼šé€šç”¨æ•°æ®è¾“å‡º
    # ========================================================================
    
    def save_general_data(self):
        """ä¿å­˜é€šç”¨é¢„å¤„ç†åæ•°æ®"""
        print("\n" + "=" * 60)
        print("ç¬¬9æ­¥ï¼šä¿å­˜é€šç”¨é¢„å¤„ç†æ•°æ®")
        print("=" * 60)
        
        df = self.df_clean
        
        # ä¿å­˜å®Œæ•´æ•°æ®
        df.to_csv(self.output_dir / 'data' / 'dwts_preprocessed_full.csv', index=False)
        
        # æŒ‰å­£èŠ‚åˆ’åˆ†æ•°æ®
        # è®­ç»ƒé›†: S3-24 (ç™¾åˆ†æ¯”åˆ¶ä¸»ä½“)
        # éªŒè¯é›†: S25-27 (ç™¾åˆ†æ¯”åˆ¶æœ«æœŸ)
        # æµ‹è¯•é›†: S28-34 (æ–°è§„åˆ™)
        
        train_df = df[df['season'].between(3, 24)]
        val_df = df[df['season'].between(25, 27)]
        test_df = df[df['season'] >= 28]
        
        train_df.to_csv(self.output_dir / 'data' / 'dwts_train_by_season.csv', index=False)
        val_df.to_csv(self.output_dir / 'data' / 'dwts_val_by_season.csv', index=False)
        test_df.to_csv(self.output_dir / 'data' / 'dwts_test_by_season.csv', index=False)
        
        print(f"âœ… é€šç”¨æ•°æ®ä¿å­˜å®Œæˆ!")
        print(f"   - å®Œæ•´æ•°æ®: {len(df)} æ¡")
        print(f"   - è®­ç»ƒé›†(S3-24): {len(train_df)} æ¡")
        print(f"   - éªŒè¯é›†(S25-27): {len(val_df)} æ¡")
        print(f"   - æµ‹è¯•é›†(S28-34): {len(test_df)} æ¡")
    
    # ========================================================================
    # ç¬¬8éƒ¨åˆ†ï¼šæ•°æ®å¯è§†åŒ–
    # ========================================================================
    
    def generate_visualizations(self):
        """ç”Ÿæˆé¢„å¤„ç†å¯è§†åŒ–å›¾è¡¨"""
        print("\n" + "=" * 60)
        print("ç¬¬10æ­¥ï¼šç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
        print("=" * 60)
        
        df = self.df_clean
        fig_dir = self.output_dir / 'figures'
        
        # è®¾ç½®å›¾è¡¨é£æ ¼
        plt.style.use('seaborn-v0_8-whitegrid')
        
        # ----- å›¾1: å­£èŠ‚æ•°æ®åˆ†å¸ƒ -----
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # æ¯å­£é€‰æ‰‹æ•°
        ax1 = axes[0, 0]
        season_counts = df.groupby('season').size()
        colors = ['#3498db' if self.get_voting_rule(s) == 'percent' else '#e74c3c' 
                  for s in season_counts.index]
        season_counts.plot(kind='bar', ax=ax1, color=colors)
        ax1.set_title('Number of Contestants per Season', fontsize=12, fontweight='bold')
        ax1.set_xlabel('Season')
        ax1.set_ylabel('Count')
        ax1.tick_params(axis='x', rotation=45)
        
        # è¯„å§”æ•°é‡åˆ†å¸ƒ
        ax2 = axes[0, 1]
        judge_counts = df.groupby('season')['judge_count'].first()
        judge_counts.plot(kind='bar', ax=ax2, color='#2ecc71')
        ax2.set_title('Judge Count per Season', fontsize=12, fontweight='bold')
        ax2.set_xlabel('Season')
        ax2.set_ylabel('Number of Judges')
        ax2.tick_params(axis='x', rotation=45)
        
        # æŠ•ç¥¨è§„åˆ™åˆ†å¸ƒ
        ax3 = axes[1, 0]
        voting_dist = df['voting_rule'].value_counts()
        voting_dist.plot(kind='pie', ax=ax3, autopct='%1.1f%%', colors=['#3498db', '#e74c3c'])
        ax3.set_title('Distribution of Voting Rules', fontsize=12, fontweight='bold')
        
        # å¹´é¾„åˆ†å¸ƒ
        ax4 = axes[1, 1]
        df['celebrity_age_during_season'].hist(ax=ax4, bins=20, color='#9b59b6', edgecolor='white')
        ax4.set_title('Age Distribution of Contestants', fontsize=12, fontweight='bold')
        ax4.set_xlabel('Age')
        ax4.set_ylabel('Frequency')
        
        plt.tight_layout()
        plt.savefig(fig_dir / 'data_overview.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        # ----- å›¾2: è¯„å§”åˆ†ä¸æ’åå…³ç³» -----
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # æŒ‰æŠ•ç¥¨è§„åˆ™åˆ†è‰²
        for rule, color, ax in zip(['percent', 'rank'], ['#3498db', '#e74c3c'], axes):
            rule_df = df[df['voting_rule'] == rule]
            ax.scatter(rule_df['avg_score_all_weeks'], rule_df['placement'], 
                      alpha=0.6, c=color, s=50)
            ax.set_xlabel('Average Judge Score', fontsize=11)
            ax.set_ylabel('Final Placement', fontsize=11)
            ax.set_title(f'{rule.title()} System: Score vs Placement', fontsize=12, fontweight='bold')
            ax.invert_yaxis()  # æ’åè¶Šå°è¶Šå¥½
            
            # æ·»åŠ ç›¸å…³ç³»æ•°
            corr = rule_df['avg_score_all_weeks'].corr(rule_df['placement'])
            ax.annotate(f'r = {corr:.3f}', xy=(0.05, 0.95), xycoords='axes fraction',
                       fontsize=11, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(fig_dir / 'score_placement_correlation.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        # ----- å›¾3: èˆä¼´å½±å“åˆ†æ -----
        fig, ax = plt.subplots(figsize=(12, 6))
        
        partner_stats = df.groupby('ballroom_partner').agg({
            'placement': 'mean',
            'season': 'count'
        }).rename(columns={'season': 'appearances'})
        
        # åªæ˜¾ç¤ºå‡ºåœºæ¬¡æ•°>=3çš„èˆä¼´
        top_partners = partner_stats[partner_stats['appearances'] >= 3].sort_values('placement')
        
        bars = ax.barh(range(len(top_partners)), top_partners['placement'], color='#1abc9c')
        ax.set_yticks(range(len(top_partners)))
        ax.set_yticklabels(top_partners.index)
        ax.set_xlabel('Average Placement (lower is better)', fontsize=11)
        ax.set_title('Professional Partners Performance (â‰¥3 appearances)', fontsize=12, fontweight='bold')
        ax.invert_xaxis()
        
        plt.tight_layout()
        plt.savefig(fig_dir / 'partner_performance.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ!")
        print(f"   - å›¾è¡¨æ•°é‡: 3")
        print(f"   - è¾“å‡ºç›®å½•: {fig_dir}")
    
    # ========================================================================
    # ä¸»æµç¨‹
    # ========================================================================
    
    def run_all(self):
        """è¿è¡Œå®Œæ•´é¢„å¤„ç†æµç¨‹"""
        print("\n" + "=" * 70)
        print("DWTS æ•°æ®é¢„å¤„ç†å®Œæ•´æµç¨‹")
        print("=" * 70)
        
        # 1. åŠ è½½æ•°æ®
        self.load_data()
        
        # 2. æ•°æ®æ¸…æ´—
        self.clean_data()
        
        # 3. é—®é¢˜ä¸€æ•°æ®å‡†å¤‡
        self.prepare_q1_constraint_optimization_data()
        self.prepare_q1_bayesian_mcmc_data()
        
        # 4. é—®é¢˜äºŒæ•°æ®å‡†å¤‡
        self.prepare_q2_kendall_bootstrap_data()
        
        # 5. é—®é¢˜ä¸‰æ•°æ®å‡†å¤‡
        self.prepare_q3_lmem_data()
        self.prepare_q3_xgboost_shap_data()
        
        # 6. é—®é¢˜å››æ•°æ®å‡†å¤‡
        self.prepare_q4_nsga2_data()
        
        # 7. ä¿å­˜é€šç”¨æ•°æ®
        self.save_general_data()
        
        # 8. ç”Ÿæˆå¯è§†åŒ–
        self.generate_visualizations()
        
        # 9. ç”Ÿæˆæ•°æ®æŠ¥å‘Š
        self._generate_report()
        
        print("\n" + "=" * 70)
        print("âœ…âœ…âœ… æ‰€æœ‰æ•°æ®é¢„å¤„ç†å®Œæˆ! âœ…âœ…âœ…")
        print("=" * 70)
        print(f"\nè¾“å‡ºç›®å½•: {self.output_dir}")
        print("\nè¾“å‡ºæ–‡ä»¶æ¸…å•:")
        self._print_output_files()
    
    def _generate_report(self):
        """ç”Ÿæˆæ•°æ®é¢„å¤„ç†æŠ¥å‘Š"""
        report = f"""
================================================================================
DWTS æ•°æ®é¢„å¤„ç†æŠ¥å‘Š
MCM 2026 Problem C
================================================================================

ä¸€ã€æ•°æ®æ¦‚è§ˆ
-----------
åŸå§‹æ•°æ®: {self.input_path}
æ•°æ®ç»´åº¦: {self.df_raw.shape[0]} è¡Œ Ã— {self.df_raw.shape[1]} åˆ—
å­£èŠ‚èŒƒå›´: S{self.df_raw['season'].min()} - S{self.df_raw['season'].max()}

äºŒã€æ•°æ®æ¸…æ´—ç»“æœ
---------------
æ¸…æ´—åç»´åº¦: {self.df_clean.shape[0]} è¡Œ Ã— {self.df_clean.shape[1]} åˆ—
æ–°å¢å­—æ®µ: judge_count, voting_rule, voting_phase, elimination_week, 
         last_valid_week, partner_id, avg_score_all_weeks, score_trend

ä¸‰ã€æŠ•ç¥¨è§„åˆ™åˆ†å¸ƒ
---------------
æ’ååˆ¶(S1-2, S28-34): {len(self.df_clean[self.df_clean['voting_rule'] == 'rank'])} æ¡
ç™¾åˆ†æ¯”åˆ¶(S3-27): {len(self.df_clean[self.df_clean['voting_rule'] == 'percent'])} æ¡

å››ã€è¾“å‡ºæ–‡ä»¶
-----------
è¯¦è§ {self.output_dir} ç›®å½•

äº”ã€å„æ¨¡å‹æ•°æ®è¯´æ˜
-----------------
é—®é¢˜ä¸€ï¼ˆçº¦æŸä¼˜åŒ–ï¼‰: models/q1_constraint_optimization_data.json
é—®é¢˜ä¸€ï¼ˆè´å¶æ–¯MCMCï¼‰: models/q1_bayesian_mcmc_data.json
é—®é¢˜äºŒï¼ˆKendall Ï„ï¼‰: models/q2_kendall_bootstrap_data.json
é—®é¢˜ä¸‰ï¼ˆLMEMï¼‰: data/q3_lmem_features.csv
é—®é¢˜ä¸‰ï¼ˆXGBoostï¼‰: data/q3_xgboost_features.csv
é—®é¢˜å››ï¼ˆNSGA-IIï¼‰: models/q4_nsga2_data.json
"""
        
        with open(self.output_dir / 'preprocessing_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
    
    def _print_output_files(self):
        """æ‰“å°è¾“å‡ºæ–‡ä»¶åˆ—è¡¨"""
        for subdir in ['data', 'models', 'figures']:
            dir_path = self.output_dir / subdir
            if dir_path.exists():
                print(f"\n  ğŸ“ {subdir}/")
                for file in sorted(dir_path.iterdir()):
                    size = file.stat().st_size / 1024  # KB
                    print(f"     ğŸ“„ {file.name} ({size:.1f} KB)")


# ============================================================================
# æ•°æ®éªŒè¯å‡½æ•°
# ============================================================================

def verify_preprocessed_data(output_dir: str):
    """
    éªŒè¯é¢„å¤„ç†åæ•°æ®çš„å®Œæ•´æ€§
    
    Args:
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
    """
    print("\n" + "=" * 60)
    print("æ•°æ®å®Œæ•´æ€§éªŒè¯")
    print("=" * 60)
    
    output_dir = Path(output_dir)
    
    required_files = [
        'data/dwts_preprocessed_full.csv',
        'data/q3_lmem_features.csv',
        'data/q3_xgboost_features.csv',
        'models/q1_constraint_optimization_data.json',
        'models/q1_bayesian_mcmc_data.json',
        'models/q2_kendall_bootstrap_data.json',
        'models/q4_nsga2_data.json'
    ]
    
    all_valid = True
    for file in required_files:
        file_path = output_dir / file
        if file_path.exists():
            size = file_path.stat().st_size / 1024
            print(f"  âœ… {file} ({size:.1f} KB)")
        else:
            print(f"  âŒ {file} - æ–‡ä»¶ç¼ºå¤±!")
            all_valid = False
    
    if all_valid:
        print("\nâœ… æ‰€æœ‰å¿…éœ€æ–‡ä»¶éªŒè¯é€šè¿‡!")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·æ£€æŸ¥é¢„å¤„ç†æµç¨‹!")
    
    return all_valid


# ============================================================================
# æ•°æ®é¢„è§ˆå‡½æ•°
# ============================================================================

def preview_data(output_dir: str):
    """
    é¢„è§ˆå¤„ç†åæ•°æ®çš„å‰10è¡Œå’Œå5è¡Œ
    
    Args:
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
    """
    output_dir = Path(output_dir)
    
    print("\n" + "=" * 60)
    print("å¤„ç†åæ•°æ®é¢„è§ˆ")
    print("=" * 60)
    
    # é¢„è§ˆå®Œæ•´æ•°æ®
    full_data_path = output_dir / 'data' / 'dwts_preprocessed_full.csv'
    if full_data_path.exists():
        df = pd.read_csv(full_data_path)
        
        print("\nğŸ“Š å®Œæ•´æ•°æ® (dwts_preprocessed_full.csv)")
        print("-" * 50)
        print(f"ç»´åº¦: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
        print("\nå‰10è¡Œ:")
        print(df.head(10).to_string())
        print("\nå5è¡Œ:")
        print(df.tail(5).to_string())


# ============================================================================
# ä¸»ç¨‹åºå…¥å£
# ============================================================================

if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘         DWTS æ•°æ®é¢„å¤„ç†ç¨‹åº                                    â•‘
    â•‘         MCM 2026 Problem C: Data With The Stars               â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # âš ï¸ è¯·ç¡®ä¿å·²ä¿®æ”¹æ–‡ä»¶é¡¶éƒ¨çš„è·¯å¾„è®¾ç½®
    print(f"ğŸ“‚ è¾“å…¥æ–‡ä»¶: {INPUT_DATA_PATH}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print("-" * 60)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(INPUT_DATA_PATH).exists():
        print(f"\nâŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ '{INPUT_DATA_PATH}'")
        print("è¯·ä¿®æ”¹æ–‡ä»¶é¡¶éƒ¨çš„ INPUT_DATA_PATH å˜é‡ä¸ºæ­£ç¡®çš„æ•°æ®æ–‡ä»¶è·¯å¾„ã€‚")
        exit(1)
    
    # åˆ›å»ºé¢„å¤„ç†å™¨å¹¶è¿è¡Œ
    preprocessor = DWTSDataPreprocessor(INPUT_DATA_PATH, OUTPUT_DIR)
    preprocessor.run_all()
    
    # éªŒè¯è¾“å‡º
    verify_preprocessed_data(OUTPUT_DIR)
    
    # é¢„è§ˆæ•°æ®
    preview_data(OUTPUT_DIR)
    
    print("\n" + "=" * 60)
    print("ç¨‹åºæ‰§è¡Œå®Œæ¯•!")
    print("=" * 60)
